#!__ENV__ __PYTHON__

#
# 1. Read every line from .lett file
# 2. For each of them, clean the text and split it in words
# 3. Lowercase and create a bag of words
# 4. Creating a list with the words corresponding to every language and a list of the documents in which these words appear
#
# Output format:
# language      word    num_doc[:inc(num_doc)]*
#
# Generates .idx -> index
#

import sys
import string
import base64
import re
from HTMLParser import HTMLParser
import argparse

reload(sys)
sys.setdefaultencoding("UTF-8")

oparser = argparse.ArgumentParser(description="Script that reads the input of bitextor-ett2lett or bitextor-lett2lettr and uses the information about the files in a crawled website to produce an index with all the words in these files and the list of documents in which each of them appear")
oparser.add_argument('lett', metavar='LETT', nargs='?', help='File produced by bitextor-ett2lett or bitextor-lett2lettr containing information about the files in the website (if undefined, the script will read from the standard input)', default=None)

options = oparser.parse_args()

if options.lett != None:
  reader = open(options.lett,"r")
else:
  reader = sys.stdin

docnumber = 0
word_map = {}
for line in reader:
  ##################
  #Parsing the text:
  ##################
  fields=line.strip().split("\t")
  lang=fields[0]
  #Decoding base 64:
  text = base64.b64decode(fields[4])
  #Removing non-aplphanumerics:
  clean_text=re.sub(r"\s+", " ", text.lower())
  clean_text=re.sub(r"([^"+string.punctuation+"])["+string.punctuation+"]+(\s)", r"\1\2", clean_text)
  clean_text=re.sub(r"(\s)["+string.punctuation+"]+([^"+string.punctuation+"])", r"\1\2", clean_text)
  clean_text.strip(string.punctuation)
  #Getting the bag of words in the document
  sorted_uniq_wordlist = set(clean_text.split())

  for word in sorted_uniq_wordlist:
    if lang in word_map:
      if word in word_map[lang]:
        word_map[lang][word].append(docnumber)
      else:
        word_map[lang][word] = []
        word_map[lang][word].append(docnumber)
    else:
      word_map[lang] = {}
      word_map[lang][word] = []
      word_map[lang][word].append(docnumber)
  docnumber=docnumber+1

for map_lang, map_vocabulary in word_map.items():
  for map_word, map_doc in map_vocabulary.items():
    sorted_docs=sorted(word_map[map_lang][map_word], reverse=True)
    for doc_list_idx in range(0, len(sorted_docs)-1):
      sorted_docs[doc_list_idx]=str(sorted_docs[doc_list_idx]-sorted_docs[doc_list_idx+1])
    sorted_docs[len(sorted_docs)-1]=str(sorted_docs[len(sorted_docs)-1])
    print map_lang+"\t"+map_word+"\t"+":".join(reversed(sorted_docs))
