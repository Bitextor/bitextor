#!__ENV__ __PYTHON__

#-*- coding: utf-8 -*-
#
# This files has been created building on the crawler.py script
# in the Creepy project, by Wei-Ning Huang (AZ) <aitjcize@gmail.com>
# and is distributed under GPLv3 license.
#

import httplib
import logging
import re
import sys

from posixpath import join, dirname, normpath
from threading import Thread, Lock
from urllib import quote, unquote

import urllib2
import base64
import argparse

reload(sys)
sys.setdefaultencoding("UTF-8")

class Document(object):
    def __init__(self, res, url):
        self.url = url
        self.query = '' if not '?' in url else url.split('?')[-1]
        self.status = res.status
        self.text = res.read()
        self.headers = dict(res.getheaders())
        self.links = []

class Crawler(object):
    F_ANY, F_SAME_DOMAIN, F_SAME_HOST, F_SAME_PATH = range(4)
    def __init__(self, debug=False):
        self.visited = {}
        self.targets = []
        self.seen = set()
        self.threads = []
        self.concurrency = 0
        self.max_outstanding = 16
        self.max_depth = 0
        self.root_url = None
        self.proto = None
        self.host = None
        self.path = None
        self.dir_path = None
        self.query = None

        self.follow_mode = self.F_SAME_HOST
        self.content_type_filter = '(text/html)'
        self.url_filters = []
        self.prefix_filter = '^(#|javascript:|mailto:)'

        self.targets_lock = Lock()
        self.concurrency_lock = Lock()

        logging.basicConfig(level=logging.DEBUG if debug else logging.ERROR)

    def set_content_type_filter(self, cf):
        self.content_type_filter = '(%s)' % ('|'.join(cf))

    def add_url_filter(self, uf):
        self.url_filters.append(uf)

    def set_follow_mode(self, mode):
        if mode > 5:
            raise RuntimeError('invalid follow mode.')
        self.follow_mode = mode

    def set_concurrency_level(self, level):
        self.max_outstanding = level

    def set_max_depth(self, max_depth):
        self.max_depth = max_depth

    def process_document(self, doc):
        print 'GET', doc.status, doc.url, doc.links
        #to do stuff with url depth use self._calc_depth(doc.url)

    def crawl(self, url):
        self.root_url = url

        rx = re.match('(https?://)([^/]+)([^\?]*)(\?.*)?', url)
        self.proto = rx.group(1)
        self.host = rx.group(2)
        self.path = rx.group(3)
        self.dir_path = dirname(self.path)
        self.query = rx.group(4)

        if url not in self.seen:
            #self.target_lock.acquire()
            self.targets.append(url)
            self.seen.add(url)
            #self.target_lock.release()
            self._spawn_new_worker()

        while self.threads:
            try:
                for t in self.threads:
                    t.join(1)
                    if not t.isAlive():
                        self.threads.remove(t)
            except KeyboardInterrupt:
                sys.exit(1)

    def _url_domain(self, host):
        parts = host.split('.')
        if len(parts) <= 2:
            return host
        elif re.match('^[0-9]+(?:\.[0-9]+){3}$', host): # IP
            return host
        else:
            return '.'.join(parts[1:])

    def _follow_link(self, url, link):
        # Remove anchor
        link = re.sub(r'#[^#]*$', '', link)

        # Skip prefix
        if re.search(self.prefix_filter, link):
            return None

        # Filter url
        for f in self.url_filters:
            if re.search(f, link):
                return None

        rx = re.match('(https?://)([^/:]+)(:[0-9]+)?([^\?]*)(\?.*)?', url)
        url_proto = rx.group(1)
        url_host = rx.group(2)
        url_port = rx.group(3) if rx.group(3) else ''
        url_path = rx.group(4) if len(rx.group(4)) > 0 else '/'
        url_dir_path = dirname(url_path)

        rx = re.match('((https?://)([^/:]+)(:[0-9]+)?)?([^\?]*)(\?.*)?', link)
        link_full_url = rx.group(1) != None
        link_proto = rx.group(2) if rx.group(2) else url_proto
        link_host = rx.group(3) if rx.group(3) else url_host
        link_port = rx.group(4) if rx.group(4) else url_port
        link_path = rx.group(5) if rx.group(5) else url_path
        link_query = quote(rx.group(6), '?=&%') if rx.group(6) else ''

        if not link_full_url and not link.startswith('/'):
            link_path = normpath(join(url_dir_path, link_path))
        link_dir_path = dirname(link_path)

        link_url = link_proto + link_host + link_port + link_path + link_query

        if self.follow_mode == self.F_ANY:
            return link_url
        elif self.follow_mode == self.F_SAME_DOMAIN:
            return link_url if self._url_domain(self.host) == \
                    self._url_domain(link_host) else None
        elif self.follow_mode == self.F_SAME_HOST:
            return link_url if self.host == link_host else None
        elif self.follow_mode == self.F_SAME_PATH:
            if self.host == link_host and \
                    link_dir_path.startswith(self.dir_path):
                return link_url
            else:
                return None

    def _calc_depth(self, url):
        # calculate url depth
        return len(url.replace('https', 'http').replace(self.root_url, '')
                .rstrip('/').split('/')) - 1

    def _add_target(self, target):
        if not target:
            return

        if self.max_depth and self._calc_depth(target) > self.max_depth:
            return

        self.targets_lock.acquire()
        if self.visited.has_key(target):
            self.targets_lock.release()
            return
        if target not in self.seen:
            self.targets.append(target)
            self.seen.add(target)
        self.targets_lock.release()

    def _spawn_new_worker(self):
        self.concurrency_lock.acquire()
        self.concurrency += 1
        t = Thread(target=self._worker, args=(self.concurrency,))
        t.daemon = True
        self.threads.append(t)
        t.start()
        self.concurrency_lock.release()

    def _worker(self, _):
        while self.targets:
            try:
                self.targets_lock.acquire()
                url = self.targets.pop(0)
                logging.debug('url: %s' % url)
                self.visited[url] = True
                self.targets_lock.release()

                rx = re.match('(https?)://([^/]+)(.*)', url)
                protocol = rx.group(1)
                host = rx.group(2)
                path = rx.group(3)

                if protocol == 'http':
                    conn = httplib.HTTPConnection(host, timeout=10)
                else:
                    conn = httplib.HTTPSConnection(host, timeout=10)

                conn.request('GET', path)
                res = conn.getresponse()

                if res.status == 301 or res.status == 302:
                    rlink = self._follow_link(url, res.getheader('location'))
                    self._add_target(rlink)
                    logging.info('redirect: %s -> %s' % (url, rlink))
                    continue

                # Check content type
                try:
                    if not re.search(self.content_type_filter,
                            res.getheader('Content-Type')):
                        sys.stderr.write(url+" discarded: wrong file type\n")
                        continue
                except TypeError: # getheader result is None
                    continue

                doc = Document(res, url)
                # Make unique list (these are the links in the document)
                links = re.findall('''href\s*=\s*['"]\s*([^'"]+)['"]''',
                        doc.text, re.S)
                doc.links = list(links)
                linksset = Set(doc.links)
                self.process_document(doc)

                for link in linksset:
                    rlink = self._follow_link(url, link.strip())
                    self._add_target(rlink)

                if self.concurrency < self.max_outstanding:
                    self._spawn_new_worker()
            except KeyError as e:
                # Pop from an empty set
                break
            except (httplib.HTTPException, EnvironmentError) as e:
                logging.error('%s: %s, retrying' % (url, str(e)))
                self.targets_lock.acquire()
                if url not in self.seen:
                    self.targets.append(url)
                    self.seen.add(url)
                self.targets_lock.release()

        self.concurrency_lock.acquire()
        self.concurrency -= 1
        self.concurrency_lock.release()

##### NEW CODE #####

oparser = argparse.ArgumentParser(description="Script that crawls a website and prints the downloaded documents in a tab-sepparated output containing the base64 encoded document and the corresponding URL.")
oparser.add_argument("URL", metavar="FILE", nargs="?", help="URL of the website to be downloaded", default=None)
options = oparser.parse_args()

class MyCrawler(Crawler):
  def process_document(self, doc):
    if doc.status == 200:
      self.concurrency_lock.acquire()
      print base64.b64encode(doc.text)+"\t"+doc.url+"\t"+str(doc.links)
      self.concurrency_lock.release()
    else:
      pass

if not options.URL.startswith("http"):
  if options.URL.find("://") != -1:
    sys.stderr.write("Error: '"+options.URL.split("://")[0]+"' is not a valid protocol; you should use either 'http' or 'https'\n")
    sys.exit(-1)

crawler = MyCrawler()
crawler.set_follow_mode(Crawler.F_SAME_DOMAIN)
#crawler.add_url_filter('\.(jpg|jpeg|gif|png|js|css|swf)$')
crawler.crawl(options.URL)
