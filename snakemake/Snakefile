import os
import sys

include: "utils.smk"

config = validate_args(config)

sys.path.append(os.path.dirname(os.path.abspath(config["bitextor"]) + "/utils"))
from utils.common import open_xz_or_gzip_or_plain

#################################################################
# BASIC PARAMETERS
BITEXTOR = config["bitextor"]
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]

LANGS = set()
LANG1 = ""
LANG2 = ""

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANG1 = config["lang1"]
    LANGS.add(LANG1)
if "lang2" in config:
    LANG2 = config["lang2"]
    LANGS.add(LANG2)

PROFILING = ""
if "profiling" in config and config["profiling"]:
    PROFILING = "/usr/bin/time -v"
#################################################################
# CRAWLING
CRAWLTARGET = ""
TLD_CRAWL = ""
USERAGENT = ""
CRAWLSIZELIMIT = ""
CRAWLTIMELIMIT = ""
CRAWLWAIT = ""
CRAWLPAGELIMIT = ""
CRAWLFILETYPES = "-f html,pdf"
CRAWLJOBS = "-j 2"
CRAWLTIMEOUT = ""
CRAWLDUMPARGS = ""
CONTINUECRAWL = ""
HERITRIXPATH = ""
HERITRIXURL = "https://localhost:8443"
HERITRIXUSER = "admin:admin"

if "crawler" in config:
    CRAWLTARGET = config["crawler"]
if "crawl-tld" in config and config["crawl-tld"]:
    TLD_CRAWL = "-D"
if "crawlerUserAgent" in config:
    USERAGENT = f'-a "{config["crawlerUserAgent"]}"'
if "crawlSizeLimit" in config:
    CRAWLSIZELIMIT = f'-s {config["crawlSizeLimit"]}'
if "crawlTimeLimit" in config:
    if CRAWLTARGET == "heritrix":
        CRAWLTIMELIMIT = config["crawlTimeLimit"]
    else:
        CRAWLTIMELIMIT = f'-t {config["crawlTimeLimit"]}'
if "crawlWait" in config:
    CRAWLWAIT = f'--wait {config["crawlWait"]}'
if "crawlFileTypes" in config:
    CRAWLFILETYPES = f'-f {config["crawlFileTypes"]}'
if "crawlerNumThreads" in config:
    CRAWLJOBS = f'-j {config["crawlerNumThreads"]}'
if "crawlerConnectionTimeout" in config:
    CRAWLTIMEOUT = f'-o {config["crawlerConnectionTimeout"]}'
if "dumpCurrentCrawl" in config:
    CRAWLDUMPARGS = f'-d {config["dumpCurrentCrawl"]}'
if "resumePreviousCrawl" in config:
    CONTINUECRAWL = f'-l {config["resumePreviousCrawl"]}'
if "heritrixPath" in config:
    HERITRIXPATH = config["heritrixPath"]
if "heritrixUrl" in config:
    HERITRIXURL = config["heritrixUrl"]
if "heritrixUser" in config:
    HERITRIXUSER = config["heritrixUser"]

#################################################################
# PREPROCESS
PPROC = "w2p"
PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]
if config["preprocessor"] == "giawarc":
    PPROC = "giawarc"
    PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz"]

SHARDS = config["shards"]
BATCHES = config["batches"]

CLEANHTML = ""
FTFY = ""
LANGID = config["langID"]
PARSER = ""
BOILERPIPE = ""
PDFEXTRACT = ""
HTML5LIB = ""

if config["cleanHTML"]:
    CLEANHTML = "--cleanhtml"
if config["ftfy"]:
    FTFY = "--ftfy"
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "boilerpipeCleaning" in config and config["boilerpipeCleaning"]==True:
    BOILERPIPE = "--boilerpipe"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT = "--pdfextract"
    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT = "--pe_configfile " + config["PDFextract_configfile"]
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT = "--sentence_join_path " + config["PDFextract_sentence_join_path"]
    else:
        PDFEXTRACT = "--sentence_join_path " + BITEXTOR + "/pdf-extract/sentence-join/sentence-join.py"
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT = "--kenlm_path " + config["PDFextract_kenlm_path"]
if "html5lib" in config and config["html5lib"]:
    HTML5LIB = "--html5lib"

# sentence splitting and tokenisation
SENTTOKS = {} if not "sentenceSplitters" in config else config["sentenceSplitters"]
CUSTOMNBPS = {} if not "customNBPs" in config else config["customNBPs"]
WORDTOKS = {} if not "wordTokenizers" in config else config["wordTokenizers"]
MORPHTOKS = {} if not "morphologicalAnalysers" in config else config["morphologicalAnalysers"]

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

#################################################################
# DOCALIGN
DOCALIGN = config["documentAligner"]
# mt
MT_COMMAND = config['alignerCmd']
SRC_LANG = LANG1
TRG_LANG = LANG2
if "translationDirection" in config and config["translationDirection"] == f'{LANG2}2{LANG1}':
    SRC_LANG = LANG2
    TRG_LANG = LANG1

DOC_THRESHOLD = 0.1
if "documentAlignerThreshold" in config:
    DOC_THRESHOLD = config["documentAlignerThreshold"]
# dic
# TODO
#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
# bleualign
BLEU_TRESHOLD = 0.1
if "sentenceAlignerThreshold" in config:
    BLEU_THRESHOLD=config["sentenceAlignerThreshold"]
# hunalign
# TODO
#################################################################
# CLEANING
FIELDS = ['url1','url2','seg1','seg2','aligner']
DEFERRED = False
DEFERRED_FIELDS = []
BIFIXER = False
BIFIXER_FIELDS = []
AGGRESSIVE_DEDUP = "--aggressive_dedup"
BICLEANER = False
BICLEANER_MODEL = ""
BICLEANER_FIELDS = []
BICLEANER_THRESHOLD = 0.0
ELRC = False
ELRC_FIELDS = []
TMX = False
DEDUPED = False
# TODO: add rawCorpus option to generate lang1-lang2.raw.gz
OUTPUT_FILES = ["sent", "raw"]

if 'deferred' in config and config['deferred']:
    DEFERRED = True
    DEFERRED_FIELDS = ['deferredseg1','checksum1','deferredseg2','checksum2']
    PARSER = "--parser 'lxml'"
if 'bifixer' in config and config['bifixer']:
    BIFIXER = True
    BIFIXER_FIELDS = ['bifixerhash','bifixerscore']
if 'aggressiveDedup' in config and not config['aggressiveDedup']:
    AGGRESSIVE_DEDUP = ''
if 'bicleaner' in config:
    BICLEANER = True
    BICLEANER_MODEL = config['bicleaner']
    BICLEANER_FIELDS = ['bicleaner']
if 'bicleanerThreshold' in config:
    BICLEANER_THRESHOLD = config['bicleanerThreshold']
if 'elrc' in config and config['elrc']:
    ELRC = True
    ELRC_FIELDS = ['lengthratio','numTokensSL','numTokensTL']
if 'tmx' in config and config['tmx']:
    TMX = True
    OUTPUT_FILES.append('not-deduped.tmx')
if 'deduped' in config and config['deduped']:
    OUTPUT_FILES.append('deduped.tmx')
    OUTPUT_FILES.append('deduped.txt')

BEFORE_ELRC_FIELDS = FIELDS + DEFERRED_FIELDS + BIFIXER_FIELDS + BICLEANER_FIELDS
TMX_FIELDS = BEFORE_ELRC_FIELDS + ELRC_FIELDS

FILTER_SORT_FIELDS="-k3,4"
TMX_DEDUP_FIELDS = 'seg1,seg2'
if 'bifixerhash' in BEFORE_ELRC_FIELDS:
    i = BEFORE_ELRC_FIELDS.index('bifixerhash')
    i = i + 1 # sort counts from 1, not 0
    FILTER_SORT_FIELDS = f'-k{i},{i} -k{i+1},{i+1}nr'
    TMX_DEDUP_FIELDS = 'bifixerhash'

BEFORE_ELRC_FIELDS = ','.join(BEFORE_ELRC_FIELDS)
TMX_FIELDS = ','.join(TMX_FIELDS)
#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])

if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])

if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())

if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())

DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)
# process WARCs individually
TARGET_2_WARCS = {f'{k}': v for k,v in enumerate(WARCS)}
# group crawled hosts by domains
TARGET_2_WARCS.update(dict([(domain, [f'{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz' for host in hosts]) for (domain, hosts) in DOMAIN_2_HOSTS.items()]))
TARGETS = TARGET_2_WARCS.keys()
#################################################################
### WORKFLOW EXECUTION ##########################################
THREADS = {'split': 1, 'translate': 1, 'tokenise_src': 1, 'tokenise_trg': 1, 'docalign': 1, 'segalign': 1, 'deferred': 1, 'bifixer': 1, 'bicleaner': 1, 'sents': 1}
if 'parallelWorkers' in config:
    for k in config['parallelWorkers']:
        THREADS[k] = config['parallelWorkers'][k]

OUTPUT = []
UNTIL = config['until'] if 'until' in config else ''
if 'until' not in config:
    OUTPUT = expand('{permanent}/{lang1}-{lang2}.{output_file}.gz', permanent=PERMANENT, target=TARGETS, lang1=LANG1, lang2=LANG2, output_file=OUTPUT_FILES)
    # this has to be added, because tokenisation rules are the last rules that don't need both shards
    # otherwise snakemake waites for both shards be completed to continue
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{TRG_LANG}')
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}')
elif UNTIL == 'crawl':
    for domain, hosts in DOMAIN_2_HOSTS.items():
        for host in hosts:
            OUTPUT.append(f'{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz')
elif UNTIL == 'preprocess':
    OUTPUT = expand('{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}', datadir=DATADIR, target=TARGETS, pproc=PPROC, langs=LANGS, pproc_file=PPROC_FILES)
elif UNTIL == 'shard':
    OUTPUT = expand('{datadir}/shards/02.batches.{lang}', datadir=DATADIR, lang=LANGS)
elif UNTIL == 'split':
    OUTPUT = expand('{datadir}/shards/03.split.{lang}', datadir=DATADIR, lang=LANGS)
elif UNTIL == 'translate':
    OUTPUT = f'{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}'
elif UNTIL == 'tokenise_trg':
    # TODO: decide where to put '0X_step' files (output of aggregate rules)
    OUTPUT = f'{DATADIR}/shards/05.tokenise.{TRG_LANG}'
elif UNTIL == 'tokenise_src':
    OUTPUT = f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}'
else:
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{TRG_LANG}')
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}')
    if UNTIL == 'docalign':
        OUTPUT.append(f'{TRANSIENT}/06_01.docalign.{SRC_LANG}_{TRG_LANG}')
    elif UNTIL == 'segalign':
        OUTPUT.append(f'{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}')
    elif UNTIL == 'bifixer':
        OUTPUT.append(f'{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}')
    elif UNTIL == 'bicleaner':
        OUTPUT.append(f'{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}')
    elif UNTIL == 'filter':
        OUTPUT.append(f'{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}')
shell.prefix("set -euo pipefail;")
rule all:
    input: OUTPUT

#################################################################
### CRAWLING ####################################################
rule creepy_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/creepy.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        {PROFILING} python3 {BITEXTOR}/bitextor-creepy.py {TLD_CRAWL} {CRAWLSIZELIMIT} {CRAWLTIMELIMIT} {CRAWLWAIT} {CRAWLJOBS} {CRAWLTIMEOUT} {CRAWLDUMPARGS} {CONTINUECRAWL} {USERAGENT} {params.url} > {output}
        '''

rule httrack_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/httrack.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d {TMPDIR}/downloaded.{wildcards.target}.XXXXXX)
        {PROFILING} {BITEXTOR}/bitextor-httrack.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {CRAWLPAGELIMIT} {USERAGENT} {CRAWLWAIT}
        {BITEXTOR}/bitextor-webdir2warc.sh $DIRNAME > {output}
        rm -rf $DIRNAME
        '''

rule wget_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/wget.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} {BITEXTOR}/bitextor-wget.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {USERAGENT} {CRAWLFILETYPES} {CRAWLWAIT} --warc {output}
        rm -rf $DIRNAME
        '''

rule heritrix_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/heritrix.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ]
            then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER}
        fi
        curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        cat {BITEXTOR}/crawler-beans.cxml | sed "s@http://example.example/example@{params.url}@g" > $DIRNAME/my-crawler-beans.cxml
        curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml
        curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        sleep 2
        curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        RUNTIME=0
        sleep 15
        while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ]
        do
            sleep 5
            RUNTIME=$((RUNTIME+5))
            if [ "{CRAWLTIMELIMIT}" != "" ]
            then
                if [ $RUNTIME -gt "{CRAWLTIMELIMIT}" ]
                then
                    echo "Crawling time limit reached"
                    curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                fi
            fi
        done
        echo "Job {wildcards.target} finished!"
        cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}
    '''
#################################################################
### PREPROCESS ##################################################

# pproc_output = {}
# for pproc_file in PPROC_FILES:
#     name = pproc_file.split('.')[0]
#     for lang in LANGS:
#         pproc_output[f"{lang}_{name}"] = f"{DATADIR}/preprocess/{{target}}/{PPROC}/{lang}/{pproc_file}"

rule warc2preprocess:
    input: lambda wildcards: TARGET_2_WARCS[wildcards.target]
    output: expand("{data}/preprocess/{{target}}/w2p/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES)
    threads: 2
    params: folder=f'{DATADIR}/preprocess/{{target}}/w2p', pproclangs=",".join(LANGS)
    shell: '''
        mkdir -p {params.folder}
        cat {input} | {PROFILING} {BITEXTOR}/bitextor-warc2htmlwarc.py {CLEANHTML} {FTFY} {PDFEXTRACT} --disable-output-gzip | {PROFILING} {BITEXTOR}/bitextor-warc2preprocess.py --input - --langs {params.pproclangs} --compression gz --langid {LANGID} {BOILERPIPE} {HTML5LIB} {PARSER} --output-dir {params.folder}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
                gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
            fi
        done
    '''

rule giawarc:
    input: lambda wildcards: TARGET_2_WARCS[wildcards.target]
    output: expand("{data}/preprocess/{{target}}/giawarc/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES)
    params: folder=f'{DATADIR}/preprocess/{{target}}/giawarc'
    shell: '''
        mkdir -p {params.folder}
        cat {input} | {PROFILING} ~/go/bin/giawarc -f bilang -l {LANGID} -o {params.folder} -
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url}}
                gzip {params.folder}/$lang/{{plain_text,mime,url}}
            fi
        done
    '''

# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    # use url.gz as input to avoid having directories as input
    input: expand("{datadir}/preprocess/{target}/{pproc}/{{lang}}/url.gz", datadir=DATADIR, target=TARGETS, pproc=PPROC)
    output: f'{DATADIR}/shards/02.batches.{{lang}}' # list of batches created for lang
    params:
        n = SHARDS,
        b = BATCHES,
        o = f'{DATADIR}/shards/{{lang}}',
        f = ','.join([f.strip(".gz") for f in PPROC_FILES])
    shell: '''
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run
        {PROFILING} ~/go/bin/giashard -n {params.n} -b {params.b} -o {params.o} -f {params.f} {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang}
        ls -d {params.o}/*/* > {output}
        '''

# obtain list of batches for lang
def get_batches(lang):
    batches = []
    with checkpoints.shard.get(lang=lang).output[0].open() as f:
        for line in f:
            batches.append(line.strip())
    return batches

rule split:
    input: f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/plain_text.gz'
    params:
        splitter = lambda wildcards: get_lang_or_default(SENTTOKS, wildcards.lang),
        customnbp = lambda wildcards: get_customnbp(CUSTOMNBPS, wildcards.lang),
    output: f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz'
    threads: THREADS['split']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        splitter=""; if [ "{params.splitter}" != "" ]; then splitter='--sentence-splitter "{params.splitter}"'; fi
        customnbp=""; if [ "{params.customnbp}" != "" ]; then customnbp='--customnbp "{params.customnbp}"'; fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-split.py \
                $splitter $customnbp \
                --langcode "{wildcards.lang}" \
                {PRUNE_THRESHOLD} {PRUNE_TYPE} \
            | pigz -c > {output}
        '''

rule aggregate_split:
    input: lambda wildcards: [f'{batch}/sentences.gz' for batch in get_batches(wildcards.lang)]
    output: f'{DATADIR}/shards/03.split.{{lang}}'
    shell: ''' echo "{input}" | tr ' ' '\n' > {output} '''

#################################################################
### DOCALIGN ####################################################
def get_align_inputs(src_lang, trg_lang):
    src_batches = get_batches(src_lang)
    trg_batches = get_batches(trg_lang)
    # each input -> (shard, (src_batch, trg_batch))
    inputs = get_mt_docalign_inputs(src_batches, trg_batches)
    return inputs

rule aggregate_matches:
    input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06_01.matches' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: f'{TRANSIENT}/06_01.docalign.{SRC_LANG}_{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''
# MT ############################################################

rule custom_translate:
    input:
        source=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz'
    threads: max(2, THREADS['translate'])
    params: THREADS['translate']
    shell: '''
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        zcat {input.source} \
            | {PROFILING} ~/go/bin/b64filter {BITEXTOR}/preprocess/bin/cache ${{parallel_cmd}} {MT_COMMAND} \
            | pigz -c > {output}
        n_before=$(zcat {input.source} | base64 -d | wc -l)
        n_after=$(zcat {output} | base64 -d | wc -l)
        # echo "Check lines count: $n_before -> $n_after for {SRC_LANG}/{wildcards.shard}/{wildcards.src_batch}"
        if [ $n_before -ne $n_after ]
        then
            >&2 echo "Lines count differ: source $n_before, target $n_after"
            exit 1
        fi
        '''
        # TODO: add early stopping

translation_output = rules.custom_translate.output

rule aggregate_translate:
    input: lambda wildcards: [f'{batch}/sentences_{TRG_LANG}.gz' for batch in get_batches(SRC_LANG)]
    output: f'{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}'
    shell: ''' echo "{input}" | tr ' ' '\n' > {output} '''

rule tokenise_translated:
    input: translation_output
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised_{TRG_LANG}.gz'
    params:
        tokeniser = lambda wildcards: get_lang_or_default(WORDTOKS, TRG_LANG),
        lemmatizer = lambda wildcards: get_lang_or_default(MORPHTOKS, TRG_LANG)
    threads: THREADS['tokenise_src']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        wordtok=""; if [ "{params.tokeniser}" != "" ]; then wordtok='--word-tokenizer "{params.tokeniser}"'; fi
        morphtok=""; if [ "{params.lemmatizer}" != "" ]; then morphtok='--morph-analyser "{params.lemmatizer}"'; fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-tokenize.py \
                $wordtok $morphtok \
                --langcode {TRG_LANG} \
            | pigz -c > {output}
        '''

rule tokenise_target:
    input: f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/tokenised.gz'
    params:
        tokeniser = lambda wildcards: get_lang_or_default(WORDTOKS, TRG_LANG),
        lemmatizer = lambda wildcards: get_lang_or_default(MORPHTOKS, TRG_LANG)
    threads: THREADS['tokenise_trg']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        wordtok=""; if [ "{params.tokeniser}" != "" ]; then wordtok='--word-tokenizer "{params.tokeniser}"'; fi
        morphtok=""; if [ "{params.lemmatizer}" != "" ]; then morphtok='--morph-analyser "{params.lemmatizer}"'; fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-tokenize.py \
                $wordtok $morphtok \
                --langcode {TRG_LANG} \
            | pigz -c > {output}
        '''

rule aggregate_tokenise_target:
    input: lambda wildcards: [f'{batch}/tokenised.gz' for batch in get_batches(TRG_LANG)]
    output: f'{DATADIR}/shards/05.tokenise.{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

rule aggregate_tokenise_source:
    input: lambda wildcards: [f'{batch}/tokenised_{TRG_LANG}.gz' for batch in get_batches(SRC_LANG)]
    output: f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

rule mt_matches:
    input:
        l1=rules.tokenise_translated.output,
        l2=rules.tokenise_target.output
    output: f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06_01.matches'
    params: folder=f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}'
    threads: THREADS['docalign']
    shell: "mkdir -p {params.folder}; {PROFILING} {BITEXTOR}/document-aligner/bin/docalign {input.l1} {input.l2} --threshold {DOC_THRESHOLD} -j {threads} > {output}"
# DIC ###########################################################
# TODO
#################################################################
### SEGALIGN ####################################################
rule aggregate_segalign:
    input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06_02.segalign.gz' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: f'{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''
# BLEUALIGN #####################################################
rule bleualign:
    input:
        indices=rules.mt_matches.output,
        plain1=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz',
        plain2=f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz',
        url1=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz',
        url2=f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz',
        translated1=translation_output
    params: folder=f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}', workers=THREADS['segalign']
    # in segalign rule output columns are reordered (or not) in accordance with translationDirection
    output: temp(f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06_02.segalign.gz')
    threads: max(THREADS['segalign'], 2)
    shell: '''
        mkdir -p {params.folder}
        parallel_cmd=""
        if [ {params.workers} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params.workers} -l 1 --group"
        fi
        cat {input.indices} \
            | {BITEXTOR}/document-aligner/bin/docjoin \
                -l {input.url1} -r {input.url2} \
                -l {input.plain1} -r {input.plain2} \
                -l {input.translated1} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bleualign-cpp/bleualign_cpp --bleu-threshold {BLEU_TRESHOLD} \
            | pigz -c > {output}
        '''
# HUNALIGN ######################################################
# TODO
#################################################################
### FILTERING AND CLEANING ######################################

# TODO: deferred_documents does not work with giawarc: html of the original document not saved
rule deferred_documents:
    input:
        html=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/normalized_html.gz',
        url=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/url.gz'
    output:
        text=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/html5lib_plain_text.xz',
        deferred=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/deferred_documents.xz'
    shell: '''
        touch {output.text}.touch && xz {output.text}.touch && mv {output.text}.touch.xz {output.text}
        touch {output.deferred}.touch && xz {output.deferred}.touch && mv {output.deferred}.touch.xz {output.deferred}
        paste <(zcat {input.html}) <(zcat {input.url}) \
            | python3 {BITEXTOR}/standoff/deferred_document.py \
            | awk -F $'\t' '{{ print $1 | "xz > {output.text}"; print $3 | "xz > {output.deferred}" }}'
        '''

deferred_input = rules.bleualign.output
# if SEGALIGN == "hunalign":
#     deferred_input = rules.hunalign.output

rule deferred_segments:
    input:
        deferred_input,
        f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/html5lib_plain_text.xz',
        f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz',
        f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/deferred_documents.xz',
        f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/html5lib_plain_text.xz',
        f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz',
        f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/deferred_documents.xz'
    output: temp(f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.07_00.deferred')
    shell: '''
        zcat {input[0]} \
            | python3 {BITEXTOR}/standoff/deferred_sentences.py <(paste <(xzcat {input[1]} {input[4]}) <(zcat {input[2]} {input[5]}) <(xzcat {input[3]} {input[6]})) \
            > {output}
        '''

split_input_filename = '06_02.segalign'
split_input_extension = '.gz'
if DEFERRED:
    split_input_filename = '07_00.deferred'
    split_input_extension = ''

# split segalign results into balanced chunks
checkpoint split_segalign:
    input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{split_input_filename}{split_input_extension}' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: batches=f'{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches'
    params:
        size=BATCHES, # use same parameter as for shards
        folder=f'{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}'
    shell: '''
        mkdir -p {params.folder}
        rm -f {params.folder}/* # remove anything that might be left after a previous run
        CAT=cat; if [[ {input[0]} == *.gz ]]; then CAT=zcat; fi
        $CAT {input} \
            | ( [ "{SRC_LANG}" = "{LANG1}" ] && cat || awk -F '\t' '{{ print $2,$1,$4,$3,$5 }}' OFS='\t' )\
            | python3 {BITEXTOR}/utils/split.py -f 3,4 -s {params.size} --gzip -o "{params.folder}/"
        ls {params.folder}/* | sed 's/.gz$//g' > {output.batches}
        '''

def get_postproc_batches():
    batches = []
    with checkpoints.split_segalign.get().output.batches.open() as f:
        for line in f:
            batches.append(line.strip().split('/')[-1]) # obtain just the number of the chunks
    return batches

rule bifixer:
    input: segalign=f'{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}/{{batch}}.gz',
    output: temp(f'{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{{batch}}')
    threads: THREADS['bifixer']
    shell: '''
        CAT=cat; if [[ {input.segalign} == *.gz ]]; then CAT=zcat; fi
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        $CAT {input.segalign} \
            | {PROFILING} ${{parallel_cmd}} python3 {BITEXTOR}/bifixer/bifixer/bifixer.py -q - - {LANG1} {LANG2} {AGGRESSIVE_DEDUP} \
            > {output}
        '''

rule aggregate_bifixer:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{batch}' for batch in get_postproc_batches()]
    output: f'{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

bicleaner_input = rules.bifixer.output
if not BIFIXER:
    bicleaner_input = rules.bifixer.input.segalign

rule bicleaner:
    input: bifixer=bicleaner_input, model=BICLEANER_MODEL
    output: f'{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{{batch}}.gz'
    params: THREADS['bicleaner']
    threads: max(2, THREADS['bicleaner'])
    shell: '''
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        CAT=cat; if [[ {input.bifixer} == *.gz ]]; then CAT=zcat; fi
        slang=$(egrep "source_lang" {input.model} | cut -d " " -f 2)
        if [ "$slang" == "{LANG1}" ]; then
            $CAT {input.bifixer} \
                | {PROFILING} {BITEXTOR}/preprocess/bin/cache -k 3,4 ${{parallel_cmd}} python3 {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        else
            $CAT {input.bifixer} \
                | awk ' BEGIN {{FS="\t"; OFS="\t"}} {{ t = $3; $3 = $4; $4 = t; print;}} ' \
                | {PROFILING} {BITEXTOR}/preprocess/bin/cache -k 3,4 ${{parallel_cmd}} python3 {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        fi
        '''

rule aggregate_bicleaner:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{batch}.gz' for batch in get_postproc_batches()]
    output: f'{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

filter_input = rules.bicleaner.output
if not BICLEANER:
    filter_input = rules.bicleaner.input.bifixer

rule filter:
    input: filter_input
    output: temp(f'{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}')
    threads: lambda wildcards: 2 if BICLEANER and ELRC else 1
    run:
        cat_cmd = "cat"
        if input[0][-3:] == ".gz":
            cat_cmd = "zcat"
        cmd = f''' {cat_cmd} {input} '''
        if BICLEANER:
            cmd += f''' | {PROFILING} python3 {BITEXTOR}/bitextor-filterbicleaner.py --threshold {BICLEANER_THRESHOLD} '''
        if ELRC:
            cmd += f''' | {PROFILING} python3 {BITEXTOR}/bitextor-elrc-filtering.py -c "{BEFORE_ELRC_FIELDS}" -s '''
        cmd += f''' | LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} ''' # sorted by either sentences or bifixer
        cmd += f''' > {output} '''
        shell(cmd)

rule aggregate_filter:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}' for batch in get_postproc_batches()]
    output: f'{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

raw_input_filename = rules.filter.input[0].split('/')[-2] # 06_02.segalign / 07_01.bifixer / 07_02.bicleaner
extension = ""
if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "07_00.deferred"]:
    extension = ".gz"

rule raw:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/{raw_input_filename}/{batch}{extension}' for batch in get_postproc_batches()]
    output:
        corpus=f'{PERMANENT}/{LANG1}-{LANG2}.raw.gz',
        stats=f'{PERMANENT}/{LANG1}-{LANG2}.stats.raw'
    shell: '''
        if [[ {input[0]} == *.gz ]]; then
            cat {input} > {output.corpus}
        else
            cat {input} | pigz -c > {output.corpus}
        fi
        echo "{LANG1}-{LANG2} raw" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.corpus} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        '''

rule sents:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}' for batch in get_postproc_batches()]
    output: f'{PERMANENT}/{LANG1}-{LANG2}.sent.gz'
    threads: THREADS['sents']
    shell: '''
        LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} --parallel {threads} --compress-program=gzip -T {TMPDIR} --merge {input} \
            | pigz -c > {output}
        '''

rule tmx:
    input: rules.sents.output
    output: f'{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz'
    shell: '''
        zcat {input} \
            | {PROFILING} python3 {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" \
            | pigz -c > {output}
        '''

rule deduped_tmx:
    input: rules.sents.output
    output:
        tmx=f'{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz',
        txt=f'{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz'
    shell: '''
        zcat {input} \
            | {PROFILING} {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" --dedup "{TMX_DEDUP_FIELDS}" -f {output.txt} \
            | pigz -c > {output.tmx}
        '''
