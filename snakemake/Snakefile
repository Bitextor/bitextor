import os
import sys

include: "utils.smk"

config = validate_args(config)

sys.path.append(os.path.dirname(os.path.abspath(config["bitextor"]) + "/utils"))
from utils.common import open_xz_or_gzip_or_plain

#################################################################
# BASIC PARAMETERS
BITEXTOR = config["bitextor"]
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]

LANGS = set()
LANG1 = ""
LANG2 = ""

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANG1 = config["lang1"]
    LANGS.add(LANG1)
if "lang2" in config:
    LANG2 = config["lang2"]
    LANGS.add(LANG2)

PROFILING = ""
if "profiling" in config and config["profiling"]:
    PROFILING = "/usr/bin/time -v"
#################################################################
# CRAWLING
CRAWLTARGET = ""
TLD_CRAWL = ""
USERAGENT = ""
CRAWLSIZELIMIT = ""
CRAWLTIMELIMIT = ""
CRAWLWAIT = ""
CRAWLPAGELIMIT = ""
CRAWLFILETYPES = "-f html,pdf"
CRAWLJOBS = "-j 2"
CRAWLTIMEOUT = ""
CRAWLDUMPARGS = ""
CONTINUECRAWL = ""
HERITRIXPATH = ""
HERITRIXURL = "https://localhost:8443"
HERITRIXUSER = "admin:admin"

if "crawler" in config:
    CRAWLTARGET = config["crawler"]
if "crawl-tld" in config and config["crawl-tld"]:
    TLD_CRAWL = "-D"
if "crawlerUserAgent" in config:
    USERAGENT = f'-a "{config["crawlerUserAgent"]}"'
if "crawlSizeLimit" in config:
    CRAWLSIZELIMIT = f'-s {config["crawlSizeLimit"]}'
if "crawlTimeLimit" in config:
    if CRAWLTARGET == "heritrix":
        CRAWLTIMELIMIT = config["crawlTimeLimit"]
    else:
        CRAWLTIMELIMIT = f'-t {config["crawlTimeLimit"]}'
if "crawlWait" in config:
    CRAWLWAIT = f'--wait {config["crawlWait"]}'
if "crawlFileTypes" in config:
    CRAWLFILETYPES = f'-f {config["crawlFileTypes"]}'
if "crawlerNumThreads" in config:
    CRAWLJOBS = f'-j {config["crawlerNumThreads"]}'
if "crawlerConnectionTimeout" in config:
    CRAWLTIMEOUT = f'-o {config["crawlerConnectionTimeout"]}'
if "dumpCurrentCrawl" in config:
    CRAWLDUMPARGS = f'-d {config["dumpCurrentCrawl"]}'
if "resumePreviousCrawl" in config:
    CONTINUECRAWL = f'-l {config["resumePreviousCrawl"]}'
if "heritrixPath" in config:
    HERITRIXPATH = config["heritrixPath"]
if "heritrixUrl" in config:
    HERITRIXURL = config["heritrixUrl"]
if "heritrixUser" in config:
    HERITRIXUSER = config["heritrixUser"]

#################################################################
# PREPROCESS
PPROC = "w2p"
PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]
if config["preprocessor"] == "giawarc":
    PPROC = "giawarc"
    PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz"]

SHARDS = config["shards"]
BATCHES = config["batches"]

CLEANHTML = ""
FTFY = ""
LANGID = config["langID"]
PARSER = ""
BOILERPIPE = ""
PDFEXTRACT = ""
HTML5LIB = ""

if config["cleanHTML"]:
    CLEANHTML = "--cleanhtml"
if config["ftfy"]:
    FTFY = "--ftfy"
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "boilerpipeCleaning" in config and config["boilerpipeCleaning"]==True:
    BOILERPIPE = "--boilerpipe"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT = "--pdfextract"
    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT = "--pe_configfile " + config["PDFextract_configfile"]
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT = "--sentence_join_path " + config["PDFextract_sentence_join_path"]
    else:
        PDFEXTRACT = "--sentence_join_path " + BITEXTOR + "/pdf-extract/sentence-join/sentence-join.py"
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT = "--kenlm_path " + config["PDFextract_kenlm_path"]
if "html5lib" in config and config["html5lib"]:
    HTML5LIB = "--html5lib"

# sentence splitting and tokenisation
SENTTOKS = {} if not "sentenceSplitters" in config else config["sentenceSplitters"]
CUSTOMNBPS = {} if not "customNBPs" in config else config["customNBPs"]
WORDTOKS = {} if not "wordTokenizers" in config else config["wordTokenizers"]
MORPHTOKS = {} if not "morphologicalAnalysers" in config else config["morphologicalAnalysers"]

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

#################################################################
# DOCALIGN
DOCALIGN = config["documentAligner"]
# mt
MT_COMMAND = config['alignerCmd']
SRC_LANG = LANG1
TRG_LANG = LANG2
if "translationDirection" in config and config["translationDirection"] == f'{LANG2}2{LANG1}':
    SRC_LANG = LANG2
    TRG_LANG = LANG1

DOC_THRESHOLD = 0.1
if "documentAlignerThreshold" in config:
    DOC_THRESHOLD = config["documentAlignerThreshold"]
# dic
# TODO
#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
# bleualign
BLEU_TRESHOLD = 0.1
if "sentenceAlignerThreshold" in config:
    BLEU_THRESHOLD=config["sentenceAlignerThreshold"]
# hunalign
# TODO
#################################################################
# CLEANING
FIELDS = ['url1','url2','seg1','seg2']
DEFERRED = False
DEFERRED_FIELDS = []
CLEAN = False
BIFIXER_FIELDS = []
AGGRESSIVE_DEDUP = "--aggressive_dedup"
BICLEANER_MODEL = ""
BICLEANER_FIELDS = []
BICLEANER_THRESHOLD = 0.5
FILTERED = "05"
ELRC_FIELDS = []

TMX = False
DEDUPED = False
OUTPUT_FILES = ["unclean_corpus"]
if 'deferred' in config and config['deferred']:
    DEFERRED = True
    DEFERRED_FIELDS = ['deferredseg1','checksum1','deferredseg2','checksum2']
    PARSER = "--parser 'lxml'"
if 'cleaning' in config and config['cleaning']:
    CLEAN = True
    BICLEANER_MODEL = config['bicleaner']
    BIFIXER_FIELDS = ['bifixerhash','bifixerscore']
    BICLEANER_FIELDS = ['bicleaner']
    ELRC_FIELDS = ['lengthratio','numTokensSL','numTokensTL']
    if 'bicleanerThreshold' in config:
        BICLEANER_THRESHOLD = config['bicleanerThreshold']
        FILTERED = str(BICLEANER_THRESHOLD).replace(".", "")
    if 'aggressiveDedup' in config and not config['aggressiveDedup']:
        AGGRESSIVE_DEDUP = ''
    OUTPUT_FILES.extend(["classified", f"filtered{FILTERED}"])
if 'tmx' in config and config['tmx']:
    TMX = True
    OUTPUT_FILES.append('not-deduped.tmx')
if 'deduped' in config and config['deduped']:
    OUTPUT_FILES.append('deduped.tmx')
    OUTPUT_FILES.append('deduped.txt')

BEFORE_ELRC_FIELDS = FIELDS + DEFERRED_FIELDS + BIFIXER_FIELDS + BICLEANER_FIELDS
TMX_FIELDS = BEFORE_ELRC_FIELDS + ELRC_FIELDS

FILTER_SORT_FIELDS="-k3,4"
TMX_DEDUP_FIELDS = 'seg1,seg2'
if 'bifixerhash' in BEFORE_ELRC_FIELDS:
    i = BEFORE_ELRC_FIELDS.index('bifixerhash')
    i = i + 1 # sort counts from 1, not 0
    FILTER_SORT_FIELDS = f'-k{i},{i} -k{i+1},{i+1}nr'
    TMX_DEDUP_FIELDS = 'bifixerhash'

BEFORE_ELRC_FIELDS = ','.join(BEFORE_ELRC_FIELDS)
TMX_FIELDS = ','.join(TMX_FIELDS)
#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])

if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])

if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())

if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())

DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)
# process WARCs individually
TARGET_2_WARCS = {f'{k}': v for k,v in enumerate(WARCS)}
# group crawled hosts by domains
TARGET_2_WARCS.update(dict([(domain, [f'{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz' for host in hosts]) for (domain, hosts) in DOMAIN_2_HOSTS.items()]))
TARGETS = TARGET_2_WARCS.keys()
#################################################################
### WORKFLOW EXECUTION ##########################################

THREADS = {'split': 1, 'translate': 1, 'tokenise': 1, 'align': 1, 'deferred': 1, 'sents': 1}
if 'parallelWorkers' in config:
    for k in config['parallelWorkers']:
        THREADS[k] = config['parallelWorkers'][k]

OUTPUT = []
UNTIL = config['until'] if 'until' in config else ''
if 'until' not in config:
    OUTPUT = expand('{permanent}/{lang1}-{lang2}.{output_file}.gz', permanent=PERMANENT, target=TARGETS, lang1=LANG1, lang2=LANG2, output_file=OUTPUT_FILES)
    # this has to be added, because tokenisation rules are the last rules that don't need both shards
    # otherwise snakemake waites for both shards be completed to continue
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{TRG_LANG}')
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}')
    print(OUTPUT)
elif UNTIL == 'crawl':
    for domain, hosts in DOMAIN_2_HOSTS.items():
        for host in hosts:
            OUTPUT.append(f'{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz')
elif UNTIL == 'preprocess':
    OUTPUT = expand('{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}', datadir=DATADIR, target=TARGETS, pproc=PPROC, langs=LANGS, pproc_file=PPROC_FILES)
elif UNTIL == 'shard':
    OUTPUT = expand('{datadir}/shards/02.batches.{lang}', datadir=DATADIR, lang=LANGS)
elif UNTIL == 'split':
    OUTPUT = expand('{datadir}/shards/03.split.{lang}', datadir=DATADIR, lang=LANGS)
elif UNTIL == 'translate':
    OUTPUT = f'{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}'
elif UNTIL == 'tokenise':
    # TODO: decide where to put '0X_step' files (output of aggregate rules)
    OUTPUT = [f'{DATADIR}/shards/05.tokenise.{TRG_LANG}', f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}']
else:
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{TRG_LANG}')
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}')
    if UNTIL == 'align':
        OUTPUT.append(f'{TRANSIENT}/06.aligned.{SRC_LANG}_{TRG_LANG}')
    elif UNTIL == 'clean':
        OUTPUT.append(f'{TRANSIENT}/08.classified.{LANG1}_{LANG2}')
        OUTPUT.append(f'{TRANSIENT}/08.filtered.{LANG1}_{LANG2}')
shell.prefix("set -euo pipefail;")
rule all:
    input: OUTPUT

#################################################################
### CRAWLING ####################################################
rule creepy_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/creepy.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        {PROFILING} python3 {BITEXTOR}/bitextor-creepy.py {TLD_CRAWL} {CRAWLSIZELIMIT} {CRAWLTIMELIMIT} {CRAWLWAIT} {CRAWLJOBS} {CRAWLTIMEOUT} {CRAWLDUMPARGS} {CONTINUECRAWL} {USERAGENT} {params.url} > {output}
        '''

rule httrack_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/httrack.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d {TMPDIR}/downloaded.{wildcards.target}.XXXXXX)
        {PROFILING} {BITEXTOR}/bitextor-httrack.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {CRAWLPAGELIMIT} {USERAGENT} {CRAWLWAIT}
        {BITEXTOR}/bitextor-webdir2warc.sh $DIRNAME > {output}
        rm -rf $DIRNAME
        '''

rule wget_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/wget.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} {BITEXTOR}/bitextor-wget.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {USERAGENT} {CRAWLFILETYPES} {CRAWLWAIT} --warc {output}
        rm -rf $DIRNAME
        '''

rule heritrix_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/heritrix.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ]
            then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER}
        fi
        curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        cat {BITEXTOR}/crawler-beans.cxml | sed "s@http://example.example/example@{params.url}@g" > $DIRNAME/my-crawler-beans.cxml
        curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml
        curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        sleep 2
        curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
        RUNTIME=0
        sleep 15
        while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ]
        do
            sleep 5
            RUNTIME=$((RUNTIME+5))
            if [ "{CRAWLTIMELIMIT}" != "" ]
            then
                if [ $RUNTIME -gt "{CRAWLTIMELIMIT}" ]
                then
                    echo "Crawling time limit reached"
                    curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                fi
            fi
        done
        echo "Job {wildcards.target} finished!"
        cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}
    '''
#################################################################
### PREPROCESS ##################################################

# pproc_output = {}
# for pproc_file in PPROC_FILES:
#     name = pproc_file.split('.')[0]
#     for lang in LANGS:
#         pproc_output[f"{lang}_{name}"] = f"{DATADIR}/preprocess/{{target}}/{PPROC}/{lang}/{pproc_file}"

rule warc2preprocess:
    input: lambda wildcards: TARGET_2_WARCS[wildcards.target]
    output: expand("{data}/preprocess/{{target}}/w2p/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES)
    threads: 2
    params: folder=f'{DATADIR}/preprocess/{{target}}/w2p', pproclangs=",".join(LANGS)
    shell: '''
        mkdir -p {params.folder}
        cat {input} | {PROFILING} {BITEXTOR}/bitextor-warc2htmlwarc.py {CLEANHTML} {FTFY} {PDFEXTRACT} --disable-output-gzip | {PROFILING} {BITEXTOR}/bitextor-warc2preprocess.py --input - --langs {params.pproclangs} --compression gz --langid {LANGID} {BOILERPIPE} {HTML5LIB} {PARSER} --output-dir {params.folder}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
                gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
            fi
        done
    '''

rule giawarc:
    input: lambda wildcards: TARGET_2_WARCS[wildcards.target]
    output: expand("{data}/preprocess/{{target}}/giawarc/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES)
    params: folder=f'{DATADIR}/preprocess/{{target}}/giawarc'
    shell: '''
        mkdir -p {params.folder}
        cat {input} | {PROFILING} ~/go/bin/giawarc -f bilang -l {LANGID} -o {params.folder} -
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url}}
                gzip {params.folder}/$lang/{{plain_text,mime,url}}
            fi
        done
    '''

# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    # use url.gz as input to avoid having directories as input
    input: expand("{datadir}/preprocess/{target}/{pproc}/{{lang}}/url.gz", datadir=DATADIR, target=TARGETS, pproc=PPROC)
    output: f'{DATADIR}/shards/02.batches.{{lang}}' # list of batches created for lang
    params:
        n = SHARDS,
        b = BATCHES,
        o = f'{DATADIR}/shards/{{lang}}',
        f = ','.join([f.strip(".gz") for f in PPROC_FILES])
    shell: '''
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run
        {PROFILING} ~/go/bin/giashard -n {params.n} -b {params.b} -o {params.o} -f {params.f} {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang}
        ls -d {params.o}/*/* > {output}
        '''

# obtain list of batches for lang
def get_batches(lang):
    batches = []
    with checkpoints.shard.get(lang=lang).output[0].open() as f:
        for line in f:
            batches.append(line.strip())
    return batches

rule split:
    input: f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/plain_text.gz'
    output: f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz'
    params:
        batch = f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}',
        splitter = lambda wildcards: get_lang_or_default(SENTTOKS, wildcards.lang),
        customnbp = lambda wildcards: get_customnbp(CUSTOMNBPS, wildcards.lang),
    shell: '''
        if [ "{params.splitter}" = "" ] && [ "{params.customnbp}" = "" ]
        then
            export PERL=perl
            export SPLIT={BITEXTOR}/preprocess/moses/ems/support/split-sentences.perl
            export TMPDIR={TMPDIR}
            mkdir -p {TMPDIR}
            bash {BITEXTOR}/scripts/03.split-text {wildcards.lang} {params.batch}
        else
            splitter=""; if [ "{params.splitter}" != "" ]; then splitter='--sentence-splitter "{params.splitter}"'; fi
            customnbp=""; if [ "{params.customnbp}" != "" ]; then customnbp='--customnbp "{params.customnbp}"'; fi
            export SPLIT={BITEXTOR}/bitextor-split.py
            export TMPDIR={TMPDIR}
            mkdir -p $TMPDIR
            bash {BITEXTOR}/scripts/03.split-text-python {wildcards.lang} {params.batch} $splitter $customnbp {PRUNE_THRESHOLD} {PRUNE_TYPE}
        fi
        '''

rule aggregate_split:
    input: lambda wildcards: [f'{batch}/sentences.gz' for batch in get_batches(wildcards.lang)]
    output: f'{DATADIR}/shards/03.split.{{lang}}'
    shell: ''' echo "{input}" | tr ' ' '\n' > {output} '''

#################################################################
### DOCALIGN ####################################################
def get_align_inputs(src_lang, trg_lang):
    src_batches = get_batches(src_lang)
    trg_batches = get_batches(trg_lang)
    # each input -> (shard, (src_batch, trg_batch))
    inputs = get_mt_docalign_inputs(src_batches, trg_batches)
    return inputs
# MT ############################################################

rule custom_translate:
    input:
        source=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz'
    threads: max(2, THREADS['translate'])
    params: THREADS['translate']
    shell: '''
        zcat {input.source} \
            | {PROFILING} {BITEXTOR}/preprocess/bin/b64filter \
                {BITEXTOR}/preprocess/bin/cache \
                    {BITEXTOR}/preprocess/bin/foldfilter -w 1000 \
                        parallel --gnu --halt 2 --pipe -j {params} -k {MT_COMMAND} \
            | pigz -c > {output}
        n_before=$(zcat {input.source} | base64 -d | wc -l)
        n_after=$(zcat {output} | base64 -d | wc -l)
        if [ $n_before -ne $n_after ]
        then
            >&2 echo "Lines count differ: source $n_before, target $n_after"
            exit 1
        fi
        '''
        # TODO: add early stopping

translation_output = rules.custom_translate.output

rule aggregate_translate:
    input: lambda wildcards: [f'{batch}/sentences_{TRG_LANG}.gz' for batch in get_batches(SRC_LANG)]
    output: f'{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}'
    shell: ''' echo "{input}" | tr ' ' '\n' > {output} '''

rule tokenise_translated:
    input: translation_output
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised_{TRG_LANG}.gz'
    threads: THREADS['tokenise']
    params:
        batch = f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}',
        tokenizer = lambda wildcards: get_lang_or_default(WORDTOKS, TRG_LANG),
        lemmatizer = lambda wildcards: get_lang_or_default(MORPHTOKS, TRG_LANG)
    shell: '''
        if [ "{params.tokenizer}" = "" ] && [ "{params.lemmatizer}" = "" ]
        then
            export THREADS={threads}
            export TARGET_LANG={TRG_LANG}
            export TOKENISE={BITEXTOR}/preprocess/moses/tokenizer/tokenizer.perl
            export TMPDIR={TMPDIR}
            export B64FILTER={BITEXTOR}/preprocess/bin/b64filter
            export CACHE={BITEXTOR}/preprocess/bin/cache
            bash {BITEXTOR}/scripts/05.tokenise {SRC_LANG} {params.batch}
        else
            export THREADS={threads}
            export TARGET_LANG={TRG_LANG}
            export TOKENISE={BITEXTOR}/bitextor-tokenize.py
            export TMPDIR={TMPDIR}
            wordtok=""; if [ "{params.tokenizer}" != "" ]; then wordtok='--word-tokenizer "{params.tokenizer}"'; fi
            morphtok=""; if [ "{params.lemmatizer}" != "" ]; then morphtok='--morph-analyser "{params.lemmatizer}"'; fi
            bash {BITEXTOR}/scripts/05.tokenise-python {SRC_LANG} {params.batch} $wordtok $morphtok
        fi
        '''

rule tokenise_target:
    input: f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/tokenised.gz'
    threads: THREADS['tokenise']
    params:
        batch = f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}',
        tokenizer = lambda wildcards: get_lang_or_default(WORDTOKS, TRG_LANG),
        lemmatizer = lambda wildcards: get_lang_or_default(MORPHTOKS, TRG_LANG)
    shell: '''
        if [ "{params.tokenizer}" = "" ] && [ "{params.lemmatizer}" = "" ]
        then
            export THREADS={threads}
            export TARGET_LANG={TRG_LANG}
            export TOKENISE={BITEXTOR}/preprocess/moses/tokenizer/tokenizer.perl
            export TMPDIR={TMPDIR}
            export B64FILTER={BITEXTOR}/preprocess/bin/b64filter
            export CACHE={BITEXTOR}/preprocess/bin/cache
            bash {BITEXTOR}/scripts/05.tokenise {TRG_LANG} {params.batch}
        else
            export THREADS={threads}
            export TARGET_LANG={TRG_LANG}
            export TOKENISE={BITEXTOR}/bitextor-tokenize.py
            export TMPDIR={TMPDIR}
            wordtok=""; if [ "{params.tokenizer}" != "" ]; then wordtok='--word-tokenizer "{params.tokenizer}"'; fi
            morphtok=""; if [ "{params.lemmatizer}" != "" ]; then morphtok='--morph-analyser "{params.lemmatizer}"'; fi
            bash {BITEXTOR}/scripts/05.tokenise-python {TRG_LANG} {params.batch} $wordtok $morphtok
        fi
        '''

rule aggregate_tokenise_target:
    input: lambda wildcards: [f'{batch}/tokenised.gz' for batch in get_batches(TRG_LANG)]
    output: f'{DATADIR}/shards/05.tokenise.{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

rule aggregate_tokenise_source:
    input: lambda wildcards: [f'{batch}/tokenised_{TRG_LANG}.gz' for batch in get_batches(SRC_LANG)]
    output: f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

rule align: # docalign + bleualign
    input:
        l1=rules.tokenise_translated.output,
        l2=rules.tokenise_target.output,
        plain1=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz',
        plain2=f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz',
        url1=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz',
        url2=f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz'
    # output: f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06.aligned.gz'
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/aligned-{TRG_LANG}{{trg_batch}}.gz'
    threads: THREADS['align']
    params:
        src_batch = f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}',
        trg_batch = f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}'
    shell: '''
        export TARGET_LANG={TRG_LANG}
        export SCRATCH={TMPDIR}
        export DOCALIGN={BITEXTOR}/document-aligner/bin/docalign
        export DOCJOIN={BITEXTOR}/document-aligner/bin/docjoin
        export BLEUALIGN={BITEXTOR}/bleualign-cpp/bleualign_cpp
        export THREADS={threads}
        bash {BITEXTOR}/scripts/06.align {SRC_LANG} {params.src_batch} {params.trg_batch}
        mv {params.src_batch}/aligned-{wildcards.trg_batch}.gz {output}
        '''
# DIC ###########################################################
# TODO
#################################################################
### SEGALIGN ####################################################
rule aggregate_align:
    # input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06.aligned.gz' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    input: lambda wildcards: [f'{DATADIR}/shards/{SRC_LANG}/{shard}/{src_batch}/aligned-{TRG_LANG}{trg_batch}.gz' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: f'{TRANSIENT}/06.aligned.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''
# BLEUALIGN #####################################################
# currently done in 'align' rule
# HUNALIGN ######################################################
# TODO
#################################################################
### FILTERING AND CLEANING ######################################

# TODO: deferred_documents does not work with giawarc: html of the original document not saved
rule deferred_documents:
    input:
        html=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/normalized_html.gz',
        url=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/url.gz'
    output:
        text=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/html5lib_plain_text.xz',
        deferred=f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/deferred_documents.xz'
    shell: '''
        touch {output.text}.touch && xz {output.text}.touch && mv {output.text}.touch.xz {output.text}
        touch {output.deferred}.touch && xz {output.deferred}.touch && mv {output.deferred}.touch.xz {output.deferred}
        paste <(zcat {input.html}) <(zcat {input.url}) \
            | python3 {BITEXTOR}/standoff/deferred_document.py \
            | awk -F $'\t' '{{ print $1 | "xz > {output.text}"; print $3 | "xz > {output.deferred}" }}'
        '''

rule deferred_segments:
    input:
        rules.align.output,
        f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/html5lib_plain_text.xz',
        f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz',
        f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/deferred_documents.xz',
        f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/html5lib_plain_text.xz',
        f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz',
        f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/deferred_documents.xz'
    output: temp(f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.07.deferred')
    shell: '''
        zcat {input[0]} \
            | python3 {BITEXTOR}/standoff/deferred_sentences.py <(paste <(xzcat {input[1]} {input[4]}) <(zcat {input[2]} {input[5]}) <(xzcat {input[3]} {input[6]})) \
            > {output}
        '''

rule unclean_corpus:
    input:
        align=lambda wildcards: [f'{DATADIR}/shards/{SRC_LANG}/{shard}/{src_batch}/aligned-{TRG_LANG}{trg_batch}.gz' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)],
        src_batches=f'{DATADIR}/shards/02.batches.{SRC_LANG}',
        trg_batches=f'{DATADIR}/shards/02.batches.{TRG_LANG}'
    output: f'{PERMANENT}/{LANG1}-{LANG2}.unclean_corpus.gz'
    shell: '''
        cat {input.src_batches} \
            | while read batch; do
                shard=$(basename $(dirname $batch))
                for match in ${{batch}}/aligned-{TRG_LANG}*.gz; do
                    echo $match 1>&2
                    # sed is here until docalign is updated
                    paste <(zcat ${{match}} \
                            | cut -f 1,2 \
                            | sed $'s/^/1.0\t/' \
                            | {BITEXTOR}/document-aligner/bin/docjoin \
                                -l $(dirname ${{match}})/url.gz \
                                -r {DATADIR}/shards/{TRG_LANG}/${{shard}}/$(echo $match | sed 's/.*-{TRG_LANG}\([0-9]\+\)\.gz/\\1/')/url.gz ) \
                        <(zcat $match | cut -f 3-)
                done
            done \
        | ( [ "{SRC_LANG}" = "{LANG1}" ] && cat || awk -F '\t' '{{ print $2,$1,$4,$3,$5 }}' OFS='\t' )\
        | gzip -c > {output}
        '''

# split segalign results into balanced chunks
checkpoint split_segalign:
    input: rules.unclean_corpus.output
    output: batches=f'{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches'
    params:
        size=BATCHES, # use same parameter as for shards
        folder=f'{TRANSIENT}/{LANG1}_{LANG2}/raws'
    shell: '''
        mkdir -p {params.folder}
        rm -f {params.folder}/* # remove anything that might be left after a previous run
        CAT=cat; if [[ {input[0]} == *.gz ]]; then CAT=zcat; fi
        $CAT {input} \
            | cut -f 1-4 \
            | python3 {BITEXTOR}/utils/split.py -f 3,4 -s {params.size} --gzip -o "{params.folder}/"
        ls {params.folder}/* | sed 's/.gz$//g' > {output.batches}
        '''

def get_postproc_batches():
    batches = []
    with checkpoints.split_segalign.get().output.batches.open() as f:
        for line in f:
            batches.append(line.strip().split('/')[-1]) # obtain just the number of the chunks
    return batches

rule clean:
    input:
        segalign=f'{TRANSIENT}/{LANG1}_{LANG2}/raws/{{batch}}.gz',
        model=BICLEANER_MODEL
    output:
        classified = f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{{batch}}.classified.gz',
        filtered = f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{{batch}}.filtered{FILTERED}.gz'
    params:
        prefix = f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{{batch}}'
    shell: '''
        export OUTPUT={params.prefix}
        export MYTEMP=$(mktemp -d {TMPDIR}/clean.XXXXXX)
        export THRESHOLD={BICLEANER_THRESHOLD}
        export L1={LANG1}
        export L2={LANG2}
        export BIFIXER={BITEXTOR}/bifixer/bifixer/bifixer.py
        export BIFIXER_PARAMS="-q {AGGRESSIVE_DEDUP}"
        export BICLEANER_LITE={BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py
        export BICLEANER_MODEL={BICLEANER_MODEL}
        slang=$(egrep "source_lang" {BICLEANER_MODEL} | cut -d " " -f 2)
        if [ "$slang" = "{SRC_LANG}" ]; then
            export BICLEANER_PARAMS="--score_only -q"
        else
            export BICLEANER_PARAMS="--score_only -q --scol 4 --tcol 3"
        fi
        export PREPROCESS_BIN={BITEXTOR}/preprocess/bin
        export FILTER_BICLEANER={BITEXTOR}/bitextor-filterbicleaner.py
        export FILTER_ELRC={BITEXTOR}/bitextor-elrc-filtering.py
        bash {BITEXTOR}/scripts/08.clean-corpus <(zcat -f {input.segalign})
        '''

rule aggregate_clean:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{{batch}}.{{f}}' for batch in get_postproc_batches() for f in ['classified.gz', f'filtered{FILTERED}.gz']]
    output:
        classified = f'{TRANSIENT}/08.classified.{LANG1}_{LANG2}',
        filtered = f'{TRANSIENT}/08.filtered.{LANG1}_{LANG2}'
    shell: '''
        echo {input} | tr ' ' '\n' | grep 'classified' > {output.classified}
        echo {input} | tr ' ' '\n' | grep 'filtered' > {output.filtered}
        '''

input_raw = None
input_sents = None

if CLEAN:
    input_raw = lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{batch}.classified.gz' for batch in get_postproc_batches()]
    input_sents = lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{batch}.filtered{FILTERED}.gz' for batch in get_postproc_batches()]
else:
    input_raw = rules.unclean_corpus.output
    input_sents = rules.unclean_corpus.output

rule reduce_classified:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{batch}.classified.gz' for batch in get_postproc_batches()]
    output:
        corpus=f'{PERMANENT}/{LANG1}-{LANG2}.classified.gz',
        stats=f'{PERMANENT}/{LANG1}-{LANG2}.stats.raw'
    params:
        input_prefix=f'{TRANSIENT}/{LANG1}_{LANG2}/clean/',
        output_prefix=f'{PERMANENT}/{LANG1}-{LANG2}'
    shell: '''
        export L={LANG1}-{LANG2}
        bash {BITEXTOR}/scripts/08.reduce-classified {params.input_prefix} {params.output_prefix}
        '''

rule reduce_filtered:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/clean/{batch}.filtered{FILTERED}.gz' for batch in get_postproc_batches()]
    params:
        input_prefix = f'{TRANSIENT}/{LANG1}_{LANG2}/clean/',
        output_prefix = f'{PERMANENT}/{LANG1}-{LANG2}'
    output: f'{PERMANENT}/{LANG1}-{LANG2}.filtered{FILTERED}.gz'
    threads: THREADS['sents']
    shell: '''
        export THRESHOLD={BICLEANER_THRESHOLD}
        export TMP=$(mktemp -d {TMPDIR}/reduce-filtered.XXXXXX)
        bash {BITEXTOR}/scripts/08.reduce-filtered {params.input_prefix} {params.output_prefix} {threads}
        rm -rf $TMP
        '''

rule tmx:
    input: rules.reduce_filtered.output
    output: f'{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz'
    shell: '''
        zcat {input} \
            | {PROFILING} python3 {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" \
            | pigz -c > {output}
        '''

rule deduped_tmx:
    input: rules.reduce_filtered.output
    output:
        tmx=f'{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz',
        txt=f'{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz',
        stats=f'{PERMANENT}/{LANG1}-{LANG2}.stats.deduped'
    shell: '''
        zcat {input} \
            | {PROFILING} {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" --dedup "{TMX_DEDUP_FIELDS}" -f {output.txt} \
            | pigz -c > {output.tmx}
        echo "{LANG1}-{LANG2} deduped txt" > {output.stats}
        echo "File size: $(du -h {output.txt} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.txt} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.txt} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        '''
