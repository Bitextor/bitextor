import gzip
import lzma
import tldextract

from tld import get_tld
from tqdm import tqdm
from collections import defaultdict
from contextlib import contextmanager
from pathlib import Path

###########################################################
# UTILS 

@contextmanager
def open_gzip_or_plain(file_path):

    def decode_text(file_handler):
        for line in file_handler:
            yield line.decode('utf-8')

    f = None
    try:
        #print("file_path", file_path)
        if file_path[-3:] == ".gz":
            f = gzip.open(file_path, 'rb')
            yield decode_text(f)
        elif file_path[-3:] == ".xz":
            f = lzma.open(file_path, 'rb')
            yield decode_text(f)
        else:
            f = open(file_path, 'r')
            yield f

    except Exception:
        raise Exception("Error occured while loading a file {}".format(file_path))

    finally:
        if f:
            f.close()


###########################################################

BITEXTOR=config["bitextor"]

LANG1=config["lang1"]
LANG2=config["lang2"]
MODEL='share/bitextor/model/keras.model'
WEIGHTS='share/bitextor/model/keras.weights'
TMPDIR=config["temp"]
MINQUALITY=config["minquality"]
MAXLINES=config["maxlines"]

#Working paths
permanent=config["permanentDir"]
transient=config["transientDir"]

#Dictionary
if "dic" in config:
  DIC=config["dic"]
else:
  DIC=None

#Option to use the NLTK tokenizer
if "nltk" in config and (config["nltk"]==True):
  USENLTK='--nltk'
else:
  USENLTK=''

#Option to ignore Boilerpipie: if the option is enabled, boilerpipe is not used
if "ignore-boilerpipe-cleaning" in config and config["ignore-boilerpipe-cleaning"]==True:
  IGNOREBOILER="1"
else:
  IGNOREBOILER="0"

#Option to use the JHU-pipeline pre-processing tool; it can be only used with HTTrack
if "jhu-lett" in config and config["jhu-lett"]==True:
  LETT="jlett"
else:
  LETT="lett"

#Option to use HTTrack for crawling instead of the native bitextor crawler
if "httrack" in config and config["httrack"]==True:
  CRAWLTARGET="httrack"
else:
  CRAWLTARGET="creepy"

############ OPTIONS FOR THE NATIVE BITEXTOR CRAWLER ############

#If this option is enabled the crawler will keep crawling across a whole top-level domain (.es, .com, .fr, etc.)
if "crawlTld" in config and config["crawlTld"]==True:
  TLD_CRAWL="-D"
else:
  TLD_CRAWL=""

#If this option is enabled, a size-limit is set for crawled data (for example "size-limit": "1G")
if "size-limit" in config:
  CRAWLSIZELIMIT="-s "+config["size-limit"]
else:
  CRAWLSIZELIMIT=""

#If this option is enabled, a time-limit is set for crawling data (for example "time-limit": "1h")
if "crawlTimeLimit" in config:
  CRAWLTIMELIMIT="-t "+config["crawlTimeLimit"]
else:
  CRAWLTIMELIMIT=""

#Option to set how many threads will be used for crawling (default value: 2). Note that too many threads can cause the server hosting the website to reject some of the simultaneous connections.
if "crawlerNumThreads" in config:
  CRAWLJOBS="-j "+str(config["crawlerNumThreads"])
else:
  CRAWLJOBS="-j 2"

#Connection timeout in the crawler
if "timeout-crawl" in config:
  CRAWLTIMEOUT="-o "+config["timeout-crawl"]
else:
  CRAWLTIMEOUT=""

#If this option is set, the "crawler" object will be dump as a pickle, so crawling can be continued afterwards
if "write-crawling-file" in config:
  CRAWLDUMPARGS="-d "+config["write-crawling-file"]
else:
  CRAWLDUMPARGS=""

#If this option is set, crawling will be continued from the pickle object dumped in a previous crawl
if "continue-crawling-file" in config:
  CONTINUECRAWL="-l "+config["continue-crawling-file"]
else:
  CONTINUECRAWL=""


############ OPTIONS FOR THE MALIGN ALIGNER ############

if "paracrawl-aligner-command" in config:
  MT_COMMAND=config["paracrawl-aligner-command"]
  DOCALIGNEXT="paracrawl"
else:
  MT_COMMAND=""
  DOCALIGNEXT="bitextor"

############ FILTERING AND POST-PROCESSING OPTIONS ############

if "bicleaner" in config:
  BICLEANEROPTION=",bicleaner"
  BICLEANER="bicleaner"
  BICLEANER_CONFIG=config["bicleaner"]
else:
  BICLEANEROPTION=""
  BICLEANER="segclean"

if "bicleanerThreshold" in config:
  BICLEANER_THRESHOLD=config["bicleanerThreshold"]
else:
  BICLEANER_THRESHOLD=0.0

if "elrc" in config and config["elrc"]==True:
  ELRCSCORES="elrc"
  ELRCFIELDS=",lengthratio,numTokensSL,numTokensTL,idnumber"
else:
  ELRCSCORES=BICLEANER
  ELRCFIELDS=""

#========================= MAPPING URLS AND OUTPUT FILES =========================#

def createDomainMap(urls):
    domains={}
    for url in urls:
        domain = tldextract.extract(url).domain
        if domain not in domains:
            domains[domain]=[]
        domains[domain].append(url)
        #print("subdomain", domain, url)
    return domains

def loadDomains(file_path):
    domains = set()
    with file_path.open("r") as f:
        for line in f:
            line = line.strip()
            if len(line):
                domains.add(line)

    return domains

def getFullDomainsFromLangstat(langstat_path, lang1, lang2, threshold, exclude_path):
    print("langstat_path", langstat_path, file=sys.stderr)
    l12 = [lang1.lower(), lang2.lower()]
    domains = defaultdict(dict)

    excluded_set = set()
    if exclude_path:
        excluded_set = loadDomains(Path(exclude_path))

    sys.stderr.write(
        "Gathering domain information for {0} and {1}...\n".format(*l12))
    with tqdm(total=None) as pbar:
        with open_gzip_or_plain(langstat_path) as f:
            for line in f:
                split_line = line.strip().split()
                if len(split_line) != 3:
                    continue

                domain, lang, byte_len = split_line
                if lang.lower() in l12:
                    domains[domain][lang.lower()] = byte_len

                pbar.update(1)
    #print("Loaded ", langstat_path, file=sys.stderr)

    domains_to_crawl = []
    for k, v in domains.items():
        if tldextract.extract(k).domain in excluded_set:
            continue

        if len(v) == 2:
            lang1_bytes = int(v[l12[0]])
            lang2_bytes = int(v[l12[1]])
            if lang1_bytes >= int(threshold) and lang2_bytes >= int(threshold):
                #print("k", k)
                domains_to_crawl.append(k)
    print(len(domains_to_crawl), " domains found", file=sys.stderr)
    return domains_to_crawl

if "domains" in config:
    print("domains given")
    domains = config["domains"]
    print("domains", type(domains))
elif "langstat" in config:
    print("find domains in langstat")
    langstat_path = config["langstat"]
    lang1 = config["lang1"]
    lang2 = config["lang2"]
    threshold = config["langstatThreshold"]
    exclude_path = config["langstatExcludeDomains"]

    domains = getFullDomainsFromLangstat(langstat_path, lang1, lang2, threshold, exclude_path)
else:
    print("Need list of urls or langstat file")
    exit()

domain_2_fulldomains = createDomainMap(domains)
#print("domain_2_fulldomains", domain_2_fulldomains)


#================================== START SNAKEMAKE================================#

#================================== TARGET FILES ==================================#

rule all:
    input:
        tmx = "{dir}/{l1}-{l2}.tmx".format(dir=permanent, l1=LANG1, l2=LANG2)
        ,
        sent = "{dir}/{l1}-{l2}.sent".format(dir=permanent, l1=LANG1, l2=LANG2)
        ,
        l1 = "{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2 = "{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        report = "{dir}/evaluation/report".format(dir=config["nmtDir"])
        ,
        reportCrawl = "{dir}/evaluation/report".format(dir=config["nmtDirCrawl"])


#================================== NMT ======================================#

rule train_nmt_with_crawl_data:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)

    output:
        report = "{dir}/evaluation/report".format(dir=config["nmtDirCrawl"])
    params:
        workDir = config["nmtDirCrawl"]
    run:
        trainData = config["nmtTrainPrefix"]

        crawlPref = "{dir}/crawl".format(dir=permanent)
        trainData.append(crawlPref)

        cmd = "snakemake --snakefile Snakefile.mt --configfile config.hieu.json -k -j3" \
            + " --directory {params.workDir}" \
            + " --config nmtTrainPrefix=\"" + str(trainData) + "\""
        #print("cmd", cmd)
        shell(cmd)

rule train_nmt:
    output:
        report = "{dir}/evaluation/report".format(dir=config["nmtDir"])
    params:
        workDir = config["nmtDir"]
    run:
        cmd = "snakemake --snakefile Snakefile.mt --configfile config.hieu.json -k -j3" \
            + " --directory {params.workDir}"
        shell(cmd)

#================================== CRAWLING ======================================#
#"http://www.elenacaffe1863.com/", "http://elenacaffe1863.com/", "http://vade-retro.fr"
rule creepy_download:
    params:
        url="http://{target}"
    output:
        '{dir}'.format(dir=permanent)+'/{target}.creepy.warc.xz'
    shell:
        'echo {params.url}'
        'mkdir -p {permanent}; '
        'python3 {BITEXTOR}/bitextor-crawl.py {TLD_CRAWL} {CRAWLSIZELIMIT} {CRAWLTIMELIMIT} {CRAWLJOBS} {CRAWLTIMEOUT} {CRAWLDUMPARGS} {CONTINUECRAWL} {params.url} | xz -c > {output}'

rule httrack_download:
    output:
        '{dir}'.format(dir=permanent)+'/{target}.httrack.warc.xz'
    params:
        url="http://{target}"
    shell:
        'mkdir -p {permanent}; '
        'DIRNAME=$(mktemp -d {TMPDIR}/downloaded_websites.XXXXXX); '
        '{BITEXTOR}/bitextor-downloadweb.sh {params.url} $DIRNAME; '
        '{BITEXTOR}/bitextor-webdir2warc.sh $DIRNAME | xz -c > {output}; '
        'rm -rf $DIRNAME;'

rule concat_subdomains:
    input:
        lambda w: expand('{dir}/{subdomain}.{crawler}.warc.xz', dir=permanent, subdomain=domain_2_fulldomains[w.target], crawler=CRAWLTARGET)
    output:
        "{dir}".format(dir=transient)+"/{target}.concat.warc.xz"
    shell:
        'xzcat {input} | xz -c > {output}'

rule warc2tt:
    input:
        '{target}.concat.warc.xz'
    output:
        '{target}.tt'
    shell:
        'xzcat {input} | {BITEXTOR}/bitextor-warc2tt.py > {output}'

rule tt2ttmime:
    input:
        '{target}.tt'
    output:
        '{target}.ttmime'
    shell:
        '{BITEXTOR}/bitextor-identifyMIME.py < {input} > {output}'

rule tt2xtt:
    input:
        #If HTTRACK is enalbed, httrack rule is run; otherwise, download rule is applied
        '{target}.ttmime'
    output:
        '{target}.xtt'
    shell:
        'java -Dfile.encoding=UTF-8 -jar {BITEXTOR}/share/java/piped-tika.jar -x < {input} > {output}'

rule xtt2boiler:
    input:
        '{target}.xtt'
    output:
        '{target}.boiler'
    shell:
        "if [ \"{IGNOREBOILER}\" == \"1\" ]; then "
        "  java -Dfile.encoding=UTF-8 -jar {BITEXTOR}/share/java/piped-boilerpipe.jar < {input} > {output}; "
        "else "
        "  cat {input} > {output}; "
        "fi"

rule boiler2ett:
    input:
        '{target}.boiler'
    output:
        '{target}.ett'
    shell:
        '{BITEXTOR}/bitextor-dedup.py < {input} > {output}'

rule ett2tika:
    input:
        '{target}.ett'
    output:
        '{target}.tika'
    shell:
        'java -Dfile.encoding=UTF-8 -jar {BITEXTOR}/share/java/piped-tika.jar -t < {input} > {output}'

rule ett2lett:
    input:
        '{target}.tika'
    output:
        '{target}.lett'
    shell:
        '{BITEXTOR}/bin/bitextor-lett-language-detector -l {LANG1},{LANG2} < {input} > {output}'

rule httrack2lett:
    output:
        '{target}.jlett'
    input:
        '{target}.tar'
    shell:
        '{BITEXTOR}/tar2lett {input} {LANG1} {LANG2} > {output}'

rule lett2lettr:
    input:
        expand("{{target}}.{extension}", extension=LETT)
    output:
        '{target}.lettr'
    shell:
        '{BITEXTOR}/bitextor-lett2lettr.py < {input} > {output}'

rule lettr2idx:
    input:
        '{target}.lettr'
    output:
        '{target}.idx'
    shell:
        '{BITEXTOR}/bitextor-lett2idx.py  --lang1 {LANG1} --lang2 {LANG2} -m 15 < {input} > {output}'




#================================== DOCUMENT ALIGNMENT ==================================#

rule idx2ridx_l1tol2:
    input:
        '{target}.idx'
    output:
        '{target}.1.ridx'
    shell:
        '{BITEXTOR}/bitextor-idx2ridx.py -d {DIC} --lang1 {LANG1} --lang2 {LANG2} < {input} > {output}'

rule idx2ridx_l2tol1:
    input:
        '{target}.idx'
    output:
        '{target}.2.ridx'
    shell:
        '{BITEXTOR}/bitextor-idx2ridx.py -d {DIC} --lang1 {LANG2} --lang2 {LANG1}  < {input} > {output}'

rule ridx2imagesetoverlap:
    input:
        '{target}.{num}.ridx',
        '{target}.lettr'
    output:
        '{target}.{num}.imgoverlap'
    shell:
        '{BITEXTOR}/features/bitextor-imagesetoverlap.py -l {wildcards.target}.lettr < {input[0]} > {output}'

rule imagesetoverlap2structuredistance:
    input:
        '{target}.{num}.imgoverlap',
        '{target}.lettr'
    output:
        '{target}.{num}.structuredistance'
    shell:
        '{BITEXTOR}/features/bitextor-structuredistance.py -l {wildcards.target}.lettr < {input[0]} > {output}'

rule structuredistance2urldistance:
    input:
        '{target}.{num}.structuredistance',
        '{target}.lettr'
    output:
        '{target}.{num}.urldistance'
    shell:
        '{BITEXTOR}/features/bitextor-urlsdistance.py -l {wildcards.target}.lettr < {input[0]} > {output}'

rule urldistance2mutuallylinked:
    input:
        '{target}.{num}.urldistance',
        '{target}.lettr'
    output:
        '{target}.{num}.mutuallylinked'
    shell:
        '{BITEXTOR}/features/bitextor-mutuallylinked.py -l {wildcards.target}.lettr < {input[0]} > {output}'

rule mutuallylinked2urlscomparison:
    input:
        '{target}.{num}.mutuallylinked',
        '{target}.lettr'
    output:
        '{target}.{num}.urlscomparison'
    shell:
        '{BITEXTOR}/features/bitextor-urlscomparison.py -l {wildcards.target}.lettr < {input[0]} > {output}'

rule urlscomparison2urlsoverlap:
    input:
        '{target}.{num}.urlscomparison',
        '{target}.lettr'
    output:
        '{target}.{num}.urlsoverlap'
    shell:
        '{BITEXTOR}/features/bitextor-urlsetoverlap.py -l {wildcards.target}.lettr < {input[0]} > {output}'

rule urlsoverlap2rank:
    input:
        '{target}.{num}.urlsoverlap',
        '{target}.lettr'
    output:
        '{target}.{num}.rank'
    shell:
        '{BITEXTOR}/bitextor-rank.py -m {BITEXTOR}/{MODEL} -w {BITEXTOR}/{WEIGHTS} < {input[0]} > {output}'

rule aligndocumentsparacrawl:
    input:
        '{target}.lett'
    output:
        '{target}.docalign.paracrawl'
    shell:
        'DOCALIGN=$(mktemp {TMPDIR}/docalign.XXXXXX); '
        '{BITEXTOR}/doc_align.sh -f {input} -l {LANG2} -t "{MT_COMMAND}" -d -w $DOCALIGN > {output}'

rule aligndocumentsbitextor:
    input:
        '{target}.1.rank',
        '{target}.2.rank',
        '{target}.lettr'
    output:
        '{target}.docalign.bitextor'
    shell:
        '{BITEXTOR}/bitextor-align-documents.py -l {input[2]} -n 1 -i converge -r /dev/null {input[0]} {input[1]} > {output}'

rule hunaligndic:
    input:
        expand("{dic}", dic=DIC)
    output:
        '{dir}/hunalign_dic'.format(dir=transient)
    shell:
        'tail -n +2 {input} | sed -r "s/\t/ @ x/g" > {output}'



#================================== SEGMENT ALIGNMENT ==================================#

rule alignsegments:
    input:
        '{dir}/hunalign_dic'.format(dir=transient),
        "{name}.docalign."+"{extension}".format(extension=DOCALIGNEXT)
    output:
        '{name}.segalign'
    shell:
        '{BITEXTOR}/bitextor-align-segments.py -d {input[0]} -t {TMPDIR} --lang1 {LANG1} --lang2 {LANG2} {USENLTK} < {input[1]} > {output}'

rule concat_segs:
    input:
        expand("{{dir}}/{webdomain}.segalign", webdomain=domain_2_fulldomains.keys())
    output:
        "{dir}/{l1}-{l2}.sentalg"
    shell:
        "cat {input} > {output}"


rule cleansegments:
    input:
        "{dir}/{l1}-{l2}.sentalg"
    output:
        "{dir}/{l1}-{l2}.segclean"
    shell:
        '{BITEXTOR}/bitextor-cleantextalign.py -q {MINQUALITY} -m {MAXLINES} -s < {input} > {output}'

#================================== POST PROCESSING ==================================#

#NOTE: did not add zipporah since it will be deprecated in version 7 of bitextor
#TODO: Add Bicleaner, add deduplication

rule bicleaner:
    input:
        "{dir}/{l1}-{l2}.segclean"
    output:
        "{dir}/{l1}-{l2}.bicleaner.scores"
    shell:
        'python3  {BITEXTOR}/bicleaner_classifier_full.py --threshold {BICLEANER_THRESHOLD} {input} {output} {BICLEANER_CONFIG}'

rule bicleanerfilter:
    input:
        "{dir}/{l1}-{l2}.bicleaner.scores"
    output:
        "{dir}/{l1}-{l2}.bicleaner"
    shell:
        '{BITEXTOR}/bitextor-filterbicleaner.py --threshold {BICLEANER_THRESHOLD} < {input} > {output}'

rule elrc:
    input:
        "{dir}/{l1}-{l2}."+"{extension}".format(extension=BICLEANER)
    output:
        "{dir}/{l1}-{l2}.elrc"
    shell:
        '{BITEXTOR}/bitextor-elrc-filtering.py -c "url1,url2,seg1,seg2,hunalign{BICLEANEROPTION}" -s < {input} > {output}'

rule sents:
    input:
        "{dir}".format(dir=config["transientDir"])+"/{l1}-{l2}."+"{ext}".format(ext=ELRCSCORES)
    output:
        "{dir}".format(dir=config["permanentDir"])+"/{l1}-{l2}.sent"
    shell:
        "cp {input} {output}"


rule tmx:
    input:
        "{dir}".format(dir=config["transientDir"])+"/{l1}-{l2}."+"{ext}".format(ext=ELRCSCORES)
    output:
        "{dir}".format(dir=config["permanentDir"])+"/{l1}-{l2}.tmx"
    shell:
        "{BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c url1,url2,seg1,seg2,hunalign{BICLEANEROPTION}{ELRCFIELDS} < {input} > {output}"

rule mt_parallel_data:
    input:
         "{dir}/{l1}-{l2}.sent".format(dir=permanent, l1=LANG1, l2=LANG2)
    output:
         l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
         ,
         l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
    shell:
         'cat {input} | cut -f 3,4 | sort | uniq > corpus; '
         "cut -f 1 corpus > {output.l1} && cut -f 2 corpus > {output.l2}"
