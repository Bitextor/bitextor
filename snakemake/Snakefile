# nohup snakemake --configfile config.hieu.json --cluster "sbatch {cluster.gres}" --cluster-config cluster.json -j 1111 --cores 1111 -k &

import gzip
import lzma
import tldextract
import os.path
from tld import get_tld
from tqdm import tqdm
from collections import defaultdict
from contextlib import contextmanager
from pathlib import Path

###########################################################
# UTILS 

def systemCheck(cmd):
    sys.stderr.write("Executing:" + cmd + "\n")
    sys.stderr.flush()

    subprocess.check_call(cmd, shell=True)

@contextmanager
def open_gzip_or_plain(file_path):

    def decode_text(file_handler):
        for line in file_handler:
            yield line.decode('utf-8')

    f = None
    try:
        #print("file_path", file_path)
        if file_path[-3:] == ".gz":
            f = gzip.open(file_path, 'rb')
            yield decode_text(f)
        elif file_path[-3:] == ".xz":
            f = lzma.open(file_path, 'rb')
            yield decode_text(f)
        else:
            f = open(file_path, 'r')
            yield f

    except Exception as ex:
        sys.stderr.write(str(ex)+"\n")
        raise Exception("Error occured while loading a file {}".format(file_path))

    finally:
        if f:
            f.close()


###########################################################

BITEXTOR=config["bitextor"]

LANG1=config["lang1"]
LANG2=config["lang2"]

if LANG1 == "en":
  FLANG=LANG2
else:
  FLANG=LANG1


TMPDIR=config["temp"]
MINQUALITY=config["minquality"]
MAXLINES=config["maxlines"]

#Working paths
permanent=config["permanentDir"]
transient=config["transientDir"]

systemCheck("mkdir -p " + permanent)
systemCheck("mkdir -p " + transient)

#Dictionary
if "dic" in config:
  DIC=config["dic"]
else:
  DIC=None

#Option to use the NLTK tokenizer
if "nltk" in config and (config["nltk"]==True):
  USENLTK='--nltk'
else:
  USENLTK=''

#Option to ignore Boilerpipie: if the option is enabled, boilerpipe is not used
if "ignore-boilerpipe-cleaning" in config and config["ignore-boilerpipe-cleaning"]==True:
  IGNOREBOILER="1"
else:
  IGNOREBOILER="0"

#Option to use the JHU-pipeline pre-processing tool; it can be only used with HTTrack
if "jhu-lett" in config and config["jhu-lett"]==True:
  LETT="jlett"
else:
  LETT="lett"

#Option to use HTTrack for crawling instead of the native bitextor crawler
if "httrack" in config and config["httrack"]==True:
  CRAWLTARGET="httrack"
else:
  CRAWLTARGET="creepy"

############ OPTIONS FOR THE NATIVE BITEXTOR CRAWLER ############

#If this option is enabled the crawler will keep crawling across a whole top-level domain (.es, .com, .fr, etc.)
if "crawlTld" in config and config["crawlTld"]==True:
  TLD_CRAWL="-D"
else:
  TLD_CRAWL=""

#If this option is enabled, a size-limit is set for crawled data (for example "size-limit": "1G")
if "size-limit" in config:
  CRAWLSIZELIMIT="-s "+config["size-limit"]
else:
  CRAWLSIZELIMIT=""

#If this option is enabled, a time-limit is set for crawling data (for example "time-limit": "1h")
if "crawlTimeLimit" in config:
  CRAWLTIMELIMIT="-t "+config["crawlTimeLimit"]
else:
  CRAWLTIMELIMIT=""

#Option to set how many threads will be used for crawling (default value: 2). Note that too many threads can cause the server hosting the website to reject some of the simultaneous connections.
if "crawlerNumThreads" in config:
  CRAWLJOBS="-j "+str(config["crawlerNumThreads"])
else:
  CRAWLJOBS="-j 2"

#Connection timeout in the crawler
if "timeout-crawl" in config:
  CRAWLTIMEOUT="-o "+config["timeout-crawl"]
else:
  CRAWLTIMEOUT=""

#If this option is set, the "crawler" object will be dump as a pickle, so crawling can be continued afterwards
if "write-crawling-file" in config:
  CRAWLDUMPARGS="-d "+config["write-crawling-file"]
else:
  CRAWLDUMPARGS=""

#If this option is set, crawling will be continued from the pickle object dumped in a previous crawl
if "continue-crawling-file" in config:
  CONTINUECRAWL="-l "+config["continue-crawling-file"]
else:
  CONTINUECRAWL=""


############ OPTIONS FOR THE MALIGN ALIGNER ############

if "paracrawl-aligner-command" in config:
  MT_COMMAND=config["paracrawl-aligner-command"]

  if MT_COMMAND == "marek-nmt":
      DOCALIGNEXT="marek-nmt"
  else:
      DOCALIGNEXT="marek-customMT"
else:
  MT_COMMAND=""
  DOCALIGNEXT="bitextor"

if "bleualign" in config and config["bleualign"]:
  SEGMENTALIGNER="bleualign"
else:
  SEGMENTALIGNER="hunalign"

#This options can be added to the config fil
DOC_THRESHOLD=0
BLEU_THRESHOLD=0

############ FILTERING AND POST-PROCESSING OPTIONS ############

if "bicleaner" in config:
  BICLEANEROPTION=",bicleaner"
  BICLEANER="bicleaner"
  BICLEANER_CONFIG=config["bicleaner"]
  BICLEANER_SORT="-r -k6,6 -k3,4"
else:
  BICLEANEROPTION=""
  BICLEANER_SORT="-k3,4"
  BICLEANER="segclean"

if "bicleanerThreshold" in config:
  BICLEANER_THRESHOLD=config["bicleanerThreshold"]
else:
  BICLEANER_THRESHOLD=0.0

if "elrc" in config and config["elrc"]==True:
  ELRCSCORES="elrc"
  ELRCFIELDS=",lengthratio,numTokensSL,numTokensTL,idnumber"
else:
  ELRCSCORES=BICLEANER
  ELRCFIELDS=""

#========================= MAPPING URLS AND OUTPUT FILES =========================#

def createDomainMap(urls):
    domains={}
    for url in urls:
        domain = tldextract.extract(url).domain
        if domain not in domains:
            domains[domain]=[]
        domains[domain].append(url)
        #print("subdomain", domain, url)
    return domains

def loadDomains(file_path):
    domains = set()
    with file_path.open("r") as f:
        for line in f:
            line = line.strip()
            if len(line):
                domains.add(line)

    return domains

def getFullDomainsFromLangstat(langstat_path, lang1, lang2, threshold, exclude_path):
    print("langstat_path", langstat_path, file=sys.stderr)
    l12 = [lang1.lower(), lang2.lower()]

    excluded_set = set()
    if exclude_path:
        excluded_set = loadDomains(Path(exclude_path))

    domains_to_crawl = []

    sys.stderr.write(
        "Gathering domain information for {0} and {1}...\n".format(*l12))
    with tqdm(total=None) as pbar:
        with open_gzip_or_plain(langstat_path) as f:

            prevDomain = ""
            langContent = {}

            for line in f:
                split_line = line.strip().split()
                if len(split_line) != 3:
                    continue

                domain, lang, byte_len = split_line
                name = tldextract.extract(domain).domain
                #print("processing ", domain, lang.lower(), byte_len, name)

                if domain != prevDomain:
                    # start of new domain. Process previous entries
                    if len(langContent) == 2:
                        lang1_bytes = langContent[l12[0]]
                        lang2_bytes = langContent[l12[1]]
                        if lang1_bytes >= threshold and lang2_bytes >= threshold:
                            domains_to_crawl.append(prevDomain)

                    prevDomain = domain
                    langContent = {}

                if lang.lower() in l12 and name not in excluded_set:
                    langContent[lang.lower()] = int(byte_len)

                pbar.update(1)

            # last domain
            if len(langContent) == 2:
                lang1_bytes = langContent[l12[0]]
                lang2_bytes = langContent[l12[1]]
                if lang1_bytes >= threshold and lang2_bytes >= threshold:
                    domains_to_crawl.append(prevDomain)


    print(len(domains_to_crawl), " domains found", file=sys.stderr)
    return domains_to_crawl

if "domains" in config:
    print("domains given")
    domains = config["domains"]
    print("domains", type(domains))
elif os.path.isfile(permanent + "/domains.gz"):
    print("read domains from file")

    with gzip.open(permanent + "/domains.gz", 'rt') as f:
        domains = f.read().splitlines()

elif "langstat" in config:
    print("find domains in langstat")
    langstat_path = config["langstat"]
    lang1 = config["lang1"]
    lang2 = config["lang2"]
    threshold = int(config["langstatThreshold"])
    exclude_path = config["langstatExcludeDomains"]

    domains = getFullDomainsFromLangstat(langstat_path, lang1, lang2, threshold, exclude_path)

    with gzip.open(permanent + "/domains.gz", 'wt') as f:
        for item in domains:
            f.write("%s\n" % item)
else:
    print("Need list of urls or langstat file")
    exit()

domain_2_fulldomains = createDomainMap(domains)
#print("domains", domains)
#print("domain_2_fulldomains", domain_2_fulldomains)

#================================== Creater Moses EMS config========================#
def createMosesEMSConfig(workDir, mosesDir, mgiza, trainPrefixes, devPrefix, testPrefixes):
    with open("{mosesDir}/scripts/ems/example/config.basic".format(mosesDir=mosesDir), "r") as file:
        lines = file.read().split("\n")
        print("lines", len(lines))
        #print("lines", lines)

    lines[8] = "working-dir = {0}".format(workDir)
    lines[11] = "input-extension = " + LANG1
    lines[12] = "output-extension = " + LANG2
    lines[13] = "#" + lines[13]

    lines[18] = "moses-src-dir = {mosesDir}".format(mosesDir=mosesDir)
    lines[27] = "external-bin-dir = {mgiza}/mgizapp/inst".format(mgiza=mgiza)
    lines[79] = "jobs = 10"
    lines[146] = "settings = \"--prune '0 0 1' -T $working-dir/lm -S 20% --discount_fallback\" "
    lines[310] = "training-options = \"-mgiza -mgiza-cpus 8\""
    lines[384] = "binarize-all = $moses-script-dir/training/binarize-model.perl"

    # DELETES
    lines[30] = ""
    lines[33] = ""
    lines[36] = ""
    lines[39] = ""
    lines[39] = ""

    # corpus
    lines[132] = lines[132] + " IGNORE"
    lines[133] = lines[133] + " IGNORE"

    # lm
    lines[206] = lines[206] + " IGNORE"
    lines[207] = lines[207] + " IGNORE"

    # tuning
    lines[522] = ""
    lines[528] = ""

    # eval
    lines[640] = "#" + lines[640]
    lines[641] = "#" + lines[641]
    lines[642] = 'multi-bleu = "$moses-script-dir/generic/multi-bleu.perl -lc"'
    lines[643] = 'multi-bleu-c = $moses-script-dir/generic/multi-bleu.perl'
    lines[662] = "#" + lines[662]
    lines[668] = "#" + lines[668]

    lines[680] = lines[680] + " IGNORE"

    # ADD
    # eval
    lines.insert(708, "")

    line = 709
    for path in testPrefixes:
        name = os.path.basename(path)
        lines.insert(line, "[EVALUATION:{name}]".format(name=name))
        lines.insert(line + 1, "raw-input = {path}.{lang}".format(path=path, lang=LANG1))
        lines.insert(line + 2, "raw-reference = {path}.{lang}".format(path=path, lang=LANG2))

        line += 3

    # tuning
    assert(len(devPrefix) == 1)
    lines[523] = "raw-input = {path}.{lang}".format(path=devPrefix[0], lang=LANG1)
    lines[529] = "raw-reference = {path}.{lang}".format(path=devPrefix[0], lang=LANG2)

    # lm
    line = 215
    for path in trainPrefixes:
        name = os.path.basename(path)
        lines.insert(line, "[LM:{name}]".format(name=name))
        lines.insert(line + 1, "raw-corpus = {path}.{lang}".format(path=path, lang=LANG2))

        line += 2

    # corpus
    lines.insert(137, "")

    line = 138
    for path in trainPrefixes:
        name = os.path.basename(path)
        lines.insert(line, "[CORPUS:{name}]".format(name=name))
        lines.insert(line + 1, "raw-stem = {path}".format(path=path))

        line += 2


    with open("{0}/steps/1/config.1".format(workDir), "w") as file:
        file.write("\n".join(lines))



#================================== START SNAKEMAKE================================#

#================================== TARGET FILES ==================================#

OUTPUT=[]

#Two WARC files with documents in lang1 and lang2
OUTPUT.append("{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1))
OUTPUT.append("{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2))

if "nmt" in config and config["nmt"]:
    OUTPUT.append("{dir}/nmt-dir/evaluation/report".format(dir=transient))
    OUTPUT.append("{dir}/nmt-dir-crawl/evaluation/report".format(dir=transient))

if "smt" in config and config["smt"]:
    OUTPUT.append("{dir}/smt-dir/evaluation/report.1".format(dir=transient))
    OUTPUT.append("{dir}/smt-dir-crawl/evaluation/report.1".format(dir=transient))

#Optional TMX: if option enabled, TMX is generated; otherwhise, tab-separated .sent file is generated
if "tmx" in config and config["tmx"]:
    if "deduped" in config and config["deduped"]:
        OUTPUT.append("{dir}/{l1}-{l2}.deduped.tmx.xz".format(dir=permanent, l1=LANG1, l2=LANG2))
    else:
        OUTPUT.append("{dir}/{l1}-{l2}.not-deduped.tmx.xz".format(dir=permanent, l1=LANG1, l2=LANG2))
else:
    OUTPUT.append("{dir}/{l1}-{l2}.sent.xz".format(dir=permanent, l1=LANG1, l2=LANG2))

rule all:
    input:
        expand("{target}", target=OUTPUT)

#================================== SMT ======================================#

rule train_smt_with_crawl_data:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        emsConfig = "{dir}/smt-dir-crawl/steps/1/config.1".format(dir=transient)

    output:
        report = "{dir}/smt-dir-crawl/evaluation/report.1".format(dir=transient)
    run:
        mosesDir = config["mosesDir"]

        cmd = "cd {transient}/smt-dir-crawl &&  {mosesDir}/scripts/ems/experiment.perl --continue 1 --exec".format(transient=transient, mosesDir=mosesDir)
        shell(cmd)

rule train_smt_with_crawl_data_create_config:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        config = "{dir}/config.json".format(dir=transient)

    output:
        emsConfig = "{dir}/smt-dir-crawl/steps/1/config.1".format(dir=transient)
    run:
        cmd = "mkdir -p {dir}/smt-dir-crawl/steps/1".format(dir=transient)
        shell(cmd)

        mosesDir = config["mosesDir"]
        trainData = config["initCorpusTrainPrefix"]

        crawlPref = "{dir}/crawl".format(dir=permanent)
        trainData.append(crawlPref)

        createMosesEMSConfig("{0}/smt-dir-crawl".format(transient),
                            mosesDir, config["mgiza"],
                            trainData,
                            config["initCorpusDevPrefix"],
                            config["initCorpusTestPrefix"])

rule train_smt_all:
    input:
        emsConfig = "{dir}/smt-dir/steps/1/config.1".format(dir=transient)

    output:
        report = "{dir}/smt-dir/evaluation/report.1".format(dir=transient)
    run:
        mosesDir = config["mosesDir"]
        cmd = "cd {transient}/smt-dir &&  {mosesDir}/scripts/ems/experiment.perl --continue 1 --exec".format(transient=transient, mosesDir=mosesDir)
        shell(cmd)

rule train_smt_all_create_config:
    input:
        config = "{dir}/config.json".format(dir=transient)

    output:
        emsConfig = "{dir}/smt-dir/steps/1/config.1".format(dir=transient)
    run:
        cmd = "mkdir -p {dir}/smt-dir/steps/1".format(dir=transient)
        shell(cmd)

        mosesDir = config["mosesDir"]

        createMosesEMSConfig("{0}/smt-dir".format(transient),
                            mosesDir, config["mgiza"],
                            config["initCorpusTrainPrefix"],
                            config["initCorpusDevPrefix"],
                            config["initCorpusTestPrefix"])

#================================== NMT ======================================#


rule train_nmt_with_crawl_data:
    input:
        l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
        ,
        l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
        ,
        config = "{dir}/config.json".format(dir=transient)

    output:
        report = "{dir}/nmt-dir-crawl/evaluation/report".format(dir=transient)

    priority: 50
    run:
        trainData = config["initCorpusTrainPrefix"]

        crawlPref = "{dir}/crawl".format(dir=permanent)
        trainData.append(crawlPref)

        cmd = "snakemake --snakefile {BITEXTOR}/snakemake/nmt/Snakefile --configfile {input.config} -k -j3" \
            + " --directory {transient}/nmt-dir-crawl" \
            + " --config initCorpusTrainPrefix=\"" + str(trainData) + "\"" \
            + " permanentDir={permanent}/nmt-dir-crawl"
        print("cmd", cmd)
        shell(cmd)

rule train_nmt_all:
    input:
        config = "{dir}/config.json".format(dir=transient)

    output:
        report = "{dir}/nmt-dir/evaluation/report".format(dir=transient)

    priority: 50
    run:
        cmd = "snakemake --snakefile {BITEXTOR}/snakemake/nmt/Snakefile --configfile {input.config} -k -j3" \
            + " --directory {transient}/nmt-dir" \
            + " --config permanentDir={permanent}/nmt-dir"
        shell(cmd)

rule create_config:
    output:
        config = temp("{dir}/config.json".format(dir=transient))
    run:
        print("Hello world")
        with open(output.config, "wt") as configFile:
            configFile.write(str(config))


#================================== CRAWLING ======================================#
#"http://www.elenacaffe1863.com/", "http://elenacaffe1863.com/", "http://vade-retro.fr"
rule creepy_download:
    params:
        url="http://{target}"
    output:
        '{dir}/warc'.format(dir=permanent)+'/{target}.creepy.warc.xz'
    priority: 10
    shell:
        'echo {params.url}'
        'mkdir -p {permanent}; '
        'python3 {BITEXTOR}/bitextor-crawl.py {TLD_CRAWL} {CRAWLSIZELIMIT} {CRAWLTIMELIMIT} {CRAWLJOBS} {CRAWLTIMEOUT} {CRAWLDUMPARGS} {CONTINUECRAWL} {params.url} | xz -c > {output}'

rule httrack_download:
    output:
        '{dir}/warc'.format(dir=permanent)+'/{target}.httrack.warc.xz'
    params:
        url="http://{target}"
    priority: 10
    shell:
        'mkdir -p {permanent}; '
        'DIRNAME=$(mktemp -d {TMPDIR}/downloaded_websites.XXXXXX); '
        '{BITEXTOR}/bitextor-downloadweb.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT}; '
        '{BITEXTOR}/bitextor-webdir2warc.sh $DIRNAME | xz -c > {output}; '
        'rm -rf $DIRNAME;'

rule concat_subdomains:
    input:
        lambda w: expand('{dir}/warc/{subdomain}.{crawler}.warc.xz', dir=permanent, subdomain=domain_2_fulldomains[w.target], crawler=CRAWLTARGET)
    output:
        "{dir}".format(dir=transient)+"/{target}.concat.warc.xz"
    shell:
        'xzcat {input} -f | xz -c > {output}'

rule warc2tt:
    input:
        '{target}.concat.warc.xz'
    output:
        '{target}.tt.xz'
    shell:
        'xzcat {input} -f | {BITEXTOR}/bitextor-warc2tt.py | xz > {output}'

rule tt2ttmime:
    input:
        '{target}.tt.xz'
    output:
        '{target}.ttmime.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-identifyMIME.py | xz > {output}'

rule tt2xtt:
    input:
        #If HTTRACK is enalbed, httrack rule is run; otherwise, download rule is applied
        '{target}.ttmime.xz'
    output:
        '{target}.xtt.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-get-html-text.py -x | xz > {output}'

#This rule won't work with current non-installation configuration
rule xtt2boiler:
    input:
        '{target}.xtt.xz'
    output:
        '{target}.boiler.xz'
    shell:
        "if [ \"{IGNOREBOILER}\" == \"1\" ]; then "
        "  xzcat -f {input} | java -Dfile.encoding=UTF-8 -jar {BITEXTOR}/share/java/piped-boilerpipe.jar | xz > {output}; "
        "else "
        "  ln -s {input} {output}; "
        "fi"

rule boiler2ett:
    input:
        '{target}.boiler.xz'
    output:
        '{target}.ett.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-dedup.py | xz > {output}'

rule ett2tika:
    input:
        '{target}.ett.xz'
    output:
        '{target}.tika.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-get-html-text.py -t | xz > {output}'

rule tika2lett:
    input:
        '{target}.tika.xz'
    output:
        '{target}.lett.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-lett-language-detector.py -l {LANG1},{LANG2} | xz > {output}'

rule httrack2lett:
    output:
        '{target}.jlett.xz'
    input:
        '{target}.tar.gz'
    shell:
        '{BITEXTOR}/tar2lett {input} {LANG1} {LANG2} | xz > {output}'

rule lett2lettr:
    input:
        expand("{{target}}.{extension}", extension=LETT)
    output:
        '{target}.lettr'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-lett2lettr.py > {output}'

rule lettr2idx:
    input:
        '{target}.lettr'
    output:
        '{target}.idx.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-lett2idx.py  --lang1 {LANG1} --lang2 {LANG2} -m 15 | xz > {output}'




#================================== DICTIONARY-BASED DOCUMENT ALIGNMENT ==================================#

rule idx2ridx_l1tol2:
    input:
        '{target}.idx.xz'
    output:
        '{target}.1.ridx.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-idx2ridx.py -d {DIC} --lang1 {LANG1} --lang2 {LANG2} | xz  > {output}'

rule idx2ridx_l2tol1:
    input:
        '{target}.idx.xz'
    output:
        '{target}.2.ridx.xz'
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-idx2ridx.py -d {DIC} --lang1 {LANG2} --lang2 {LANG1} | xz > {output}'

rule ridx2imagesetoverlap:
    input:
        '{target}.{num}.ridx.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.imgoverlap.xz'
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/features/bitextor-imagesetoverlap.py -l {wildcards.target}.lettr | xz > {output}'

rule imagesetoverlap2structuredistance:
    input:
        '{target}.{num}.imgoverlap.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.structuredistance.xz'
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/features/bitextor-structuredistance.py -l {wildcards.target}.lettr | xz > {output}'

rule structuredistance2urldistance:
    input:
        '{target}.{num}.structuredistance.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.urldistance.xz'
    priority: 8
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/features/bitextor-urlsdistance.py -l {wildcards.target}.lettr | xz > {output}'

rule urldistance2mutuallylinked:
    input:
        '{target}.{num}.urldistance.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.mutuallylinked.xz'
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/features/bitextor-mutuallylinked.py -l {wildcards.target}.lettr | xz > {output}'

rule mutuallylinked2urlscomparison:
    input:
        '{target}.{num}.mutuallylinked.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.urlscomparison.xz'
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/features/bitextor-urlscomparison.py -l {wildcards.target}.lettr | xz > {output}'

rule urlscomparison2urlsoverlap:
    input:
        '{target}.{num}.urlscomparison.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.urlsoverlap.xz'
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/features/bitextor-urlsetoverlap.py -l {wildcards.target}.lettr | xz > {output}'

rule urlsoverlap2rank:
    input:
        '{target}.{num}.urlsoverlap.xz',
        '{target}.lettr'
    output:
        '{target}.{num}.rank.xz'
    shell:
        'xzcat -f {input[0]} | {BITEXTOR}/bitextor-rank.py -m {BITEXTOR}/share/bitextor/model/keras.model -w {BITEXTOR}/share/bitextor/model/keras.weights | xz > {output}'

rule aligndocumentsBitextor:
    input:
        '{target}.1.rank.xz',
        '{target}.2.rank.xz',
        '{target}.lettr'
    output:
        '{target}.docalign.bitextor'
    shell:
        '{BITEXTOR}/bitextor-align-documents.py -l {input[2]} -n 1 -i converge -r /dev/null {input[0]} {input[1]} > {output}'



################# MT-BASED DOCUMENT ALIGNMENT #################

rule docaling_extracted:
    input:
        "{prefix}.lett.xz"
    output:
        "{prefix}.docalign/"+"{l1}.extracted.xz".format(l1=LANG1),
        "{prefix}.docalign/"+"{l2}.extracted.xz".format(l2=LANG2)
    shell:
        'mkdir -p {wildcards.prefix}.docalign; '
        'xzcat -f {input} | {BITEXTOR}/document-aligner/utils/extract_lett.py -x --langs {LANG1},{LANG2} --splitter {BITEXTOR}/preprocess/moses/ems/support/split-sentences.perl --prune_type "words" --prune 80 --output_dir {wildcards.prefix}.docalign'

rule docaling_deduped:
    input:
        "{target}/{lang}.extracted.xz"
    output:
        temp("{target}/{lang}.extracted.deduped.xz")
    shell:
        "xzcat {input}  | cut -d$'\t' -f 2 | sort | uniq | xz -c > {output}"

rule docaling_translated:
    input:
        source = "{dir}/{prefix}.extracted.deduped.xz"
        ,
        config = "{dir}/config.json".format(dir=transient)

    output:
        temp("{dir}/{prefix}.marek-nmt.extracted.deduped.translated.xz")

    priority: 40

    shell:
        'xzcat {input.source} | {BITEXTOR}/snakemake/nmt/translate.sh {input.config} {wildcards.dir} {permanent}/nmt-dir | xz -c > {output}; '
        'if [ "$(xzcat {input} | wc -l)" -ne "$(xzcat {output} | wc -l)" ]; then >&2 echo "TRANSLATION ERROR (command {MT_COMMAND}): {input} and {output} should have the same number of lines"; exit 2; fi'


rule docaling_custom_translated:
    input:
        "{target}.extracted.deduped.xz",
    output:
        temp("{target}.marek-customMT.extracted.deduped.translated.xz")
    shell:
        'xzcat {input} | {MT_COMMAND} | xz -c > {output}; '
        'if [ "$(xzcat {input} | wc -l)" -ne "$(xzcat {output} | wc -l)" ]; then >&2 echo "TRANSLATION ERROR (command {MT_COMMAND}): {input} and {output} should have the same number of lines"; exit 2; fi'


rule docaling_substitute_translated:
    input:
        extracted="{target}.extracted.xz",
        deduped="{target}.extracted.deduped.xz",
        translated="{target}.{mttype}.extracted.deduped.translated.xz"
    output:
        "{target}.{mttype}.extracted.translated.xz"
    shell:
        'xzcat {input.extracted} | python3 {BITEXTOR}/document-aligner/substitute_translated.py --deduplicated {input.deduped} --translated {input.translated} | xz -c > {output}'

rule docaling_matches:
    input:
        l1="{dir}/"+"{l1}".format(l1=LANG1)+".{mttype}.extracted.translated.xz",
        l2="{dir}/"+"{l2}.extracted.xz".format(l2=LANG2)
    output:
        "{dir}/"+"{l1}-{l2}".format(l1=LANG1,l2=LANG2)+".{mttype}.matches"
    shell:
        "python3 {BITEXTOR}/document-aligner/compute_matches.py --lang1 {input.l1} --lang2 {input.l2} --output_matches {output} --threshold {DOC_THRESHOLD}"

rule aligndocumentsMarek:
    input:
        lett="{target}.lett.xz",
        matches="{target}.docalign/"+"{l1}-{l2}.{mttype}.matches".format(l1=LANG1,l2=LANG2,mttype=DOCALIGNEXT),
        report="{dir}/nmt-dir/evaluation/report".format(dir=transient)
    output:
        '{target}.docalign.marek-nmt'

    shell:
        'mkdir -p {wildcards.target}.docalign; '
        'xzcat -f {input.lett} | python3 {BITEXTOR}/document-aligner/build_docs.py --matches {input.matches} --threshold {DOC_THRESHOLD} > {output}'

rule aligndocumentsMarekCustomMT:
    input:
        lett="{target}.lett.xz",
        matches="{target}.docalign/"+"{l1}-{l2}.{mttype}.matches".format(l1=LANG1,l2=LANG2,mttype=DOCALIGNEXT)
    output:
        '{target}.docalign.marek-customMT'
    shell:
        'xzcat -f {input.lett} | python3 {BITEXTOR}/document-aligner/build_docs.py --matches {input.matches} --threshold {DOC_THRESHOLD} > {output}'


#================================== SEGMENT ALIGNMENT ==================================#

rule hunaligndic:
    input:
        expand("{dic}", dic=DIC)
    output:
        '{dir}/hunalign_dic'.format(dir=transient)
    shell:
        'tail -n +2 {input} | sed -r "s/\t/ @ x/g" > {output}'

rule alignsegments_hunalign:
    input:
        '{dir}/hunalign_dic'.format(dir=transient),
        "{name}.docalign."+"{extension}".format(extension=DOCALIGNEXT)
    output:
        '{name}.hunalign.segalign.xz'
    shell:
        'xzcat -f {input[1]} | {BITEXTOR}/bitextor-align-segments.py -d {input[0]} -t {TMPDIR} --lang1 {LANG1} --lang2 {LANG2} {USENLTK} --hunalign-dir "{BITEXTOR}/bin" | xz > {output}'

rule alignsegments_bleualign:
    input:
        aligned_urls="{prefix}.bleualign/align.info.xz"
    output:
        alignments='{prefix}.bleualign.segalign.xz'
    run:
        with lzma.open(output.alignments, "wt") as algFile:
            with open_gzip_or_plain(input.aligned_urls) as urlsFile:
                for urlAlg in urlsFile:
                    id=urlAlg.split("\t")[0]
                    urls="\t".join(urlAlg.strip().split("\t")[1:])
                    filename=wildcards.prefix+".bleualign/aligned."+str(id)+".xz"
                    with open_gzip_or_plain(filename) as sentFile:
                        for line in sentFile:
                            algFile.write(urls+"\t"+line)

#rule concat_segs:
#    input:
#        expand("{{dir}}/{webdomain}.{segaligner}.segalign.xz", webdomain=domain_2_fulldomains.keys(), segaligner={SEGMENTALIGNER})
#    output:
#        "{dir}/{l1}-{l2}.sentalg.xz"
#    shell:
#        "xzcat {input} | xz > {output}"

rule cleansegments:
    input:
        "{target}.segalign.xz"
    output:
        "{target}.segclean.xz"
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-cleantextalign.py -q {MINQUALITY} -m {MAXLINES} -s | xz > {output}'

#================================== POST PROCESSING ==================================#

#NOTE: did not add zipporah since it will be deprecated in version 7 of bitextor
#TODO: Add Bicleaner, add deduplication

rule bicleaner:
    input:
        "{target}.segclean.xz"
    output:
        "{target}.bicleaner.scores.xz"
    shell:
        'slang=$(egrep "source_lang" {BICLEANER_CONFIG} | cut -d " " -f 2); '
        'if [ "$slang" == "{LANG1}" ]; then '
        '  xzcat -f {input} | python3  {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_full.py --threshold {BICLEANER_THRESHOLD} - - {BICLEANER_CONFIG} | xz > {output}; '
        'else '
        '  xzcat -f {input} | awk \' BEGIN {{FS="\t"; OFS="\t"}} {{ print $1 "\t" $2 "\t" $4 "\t" $3 "\t" $5}}\' | python3  {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_full.py --threshold {BICLEANER_THRESHOLD} - - {BICLEANER_CONFIG}  | awk \' BEGIN {{FS="\t"; OFS=","}} {{ print $1 "\t" $2 "\t" $4 "\t" $3 "\t" $5 "\t" $6}}\' | xz > {output}; '
        'fi'

rule bicleanerfilter:
    input:
        "{target}.bicleaner.scores.xz"
    output:
        "{target}.bicleaner.xz"
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-filterbicleaner.py --threshold {BICLEANER_THRESHOLD} | xz > {output}'

rule elrc:
    input:
        "{target}."+"{extension}".format(extension=BICLEANER)+".xz"
    output:
        "{target}.elrc.xz"
    shell:
        'xzcat -f {input} | {BITEXTOR}/bitextor-elrc-filtering.py -c "url1,url2,seg1,seg2,hunalign{BICLEANEROPTION}" -s | xz > {output}'

rule sents:
    input:
        expand("{dir}/{webdomain}.{aligner}.{ext}.xz", dir=config["transientDir"], webdomain=domain_2_fulldomains.keys(), aligner=SEGMENTALIGNER, ext={ELRCSCORES})
    output:
        "{dir}".format(dir=config["permanentDir"])+"/{l1}-{l2}.sent.xz"
    shell:
        "xzcat {input} | xz > {output}"

rule tmx:
    input:
        expand("{dir}/{webdomain}.{aligner}.{ext}.xz", dir=config["transientDir"], webdomain=domain_2_fulldomains.keys(), aligner=SEGMENTALIGNER, ext={ELRCSCORES})
    output:
        "{dir}".format(dir=config["permanentDir"])+"/{l1}-{l2}.not-deduped.tmx.xz"
    shell:
        "xzcat -f {input} | {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c url1,url2,seg1,seg2,hunalign{BICLEANEROPTION}{ELRCFIELDS} | xz > {output}"

rule deduped_tmx:
    input:
        expand("{dir}/{webdomain}.{aligner}.{ext}.xz", dir=config["transientDir"], webdomain=domain_2_fulldomains.keys(), aligner=SEGMENTALIGNER, ext={ELRCSCORES})
    output:
        "{dir}".format(dir=config["permanentDir"])+"/{l1}-{l2}.deduped.tmx.xz"
    shell:
        "xzcat -f {input} | LC_ALL=C sort -t$'\t' {BICLEANER_SORT} --compress-program=gzip | {BITEXTOR}/bitextor-buildTMX-dedup.py --lang1 {LANG1} --lang2 {LANG2} -c url1,url2,seg1,seg2,hunalign{BICLEANEROPTION}{ELRCFIELDS} | xz > {output}"

rule mt_parallel_data:
    input:
         "{dir}/{l1}-{l2}.sent".format(dir=permanent, l1=LANG1, l2=LANG2)+".xz"
    output:
         l1="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG1)
         ,
         l2="{dir}/crawl.{lang}".format(dir=permanent, lang=LANG2)
    shell:
         "xzcat -f {input} | cut -f 3,4 | sort | uniq > corpus; "
         "cut -f 1 corpus > {output.l1} && cut -f 2 corpus > {output.l2}; "
         "rm corpus"

################# BLEUALIGN RULES #################

rule bleualign:
    input:
        extracted_l1="{prefix}.docalign/"+"{l1}.extracted.xz".format(l1=LANG1),
        extracted_l2="{prefix}.docalign/"+"{l2}.extracted.xz".format(l2=LANG2),
        translated="{prefix}.docalign/"+"{l1}.{mttype}.extracted.translated.xz".format(l1=LANG1,mttype=DOCALIGNEXT),
        matches="{prefix}.docalign/"+"{l1}-{l2}.{mttype}.matches".format(l1=LANG1,l2=LANG2,mttype=DOCALIGNEXT)
    output:
        "{prefix}.bleualign/align.info.xz"
    shell:
        "mkdir -p {wildcards.prefix}.bleualign; "
        "{BITEXTOR}/bleualign-cpp/bleualign_cpp --text1 {input.extracted_l1} --text2 {input.extracted_l2} --text2translated {input.translated} --matches {input.matches} --doc-threshold {DOC_THRESHOLD} --bleu-threshold {BLEU_THRESHOLD} --output-dir {wildcards.prefix}.bleualign"
