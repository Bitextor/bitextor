#!__ENV__ __PYTHON__
# -*- coding: utf-8 -*-

#
# 1. Read every line from .lett file
# 2. For each of them, clean the text and split it in words
# 3. Lowercase and create a bag of words
# 4. Creating a list with the words corresponding to every language and a list of the documents in which these words appear
#
# Output format:
# language      word    num_doc[:inc(num_doc)]*
#
# Generates .idx -> index
#

import sys
import string
import base64
from HTMLParser import HTMLParser
import argparse
import unicodedata
sys.path.append("__PREFIX__/share/bitextor/utils")
from unicodepunct import get_unicode_punct
#from nltk.tokenize.punkt import PunktWordTokenizer
from nltk import word_tokenize
from collections import Counter
from sets import Set
import math

reload(sys)
sys.setdefaultencoding("UTF-8")

oparser = argparse.ArgumentParser(description="Script that reads the input of bitextor-ett2lett or bitextor-lett2lettr and uses the information about the files in a crawled website to produce an index with all the words in these files and the list of documents in which each of them appear")
oparser.add_argument('lett', metavar='LETT', nargs='?', help='File produced by bitextor-ett2lett or bitextor-lett2lettr containing information about the files in the website (if undefined, the script will read from the standard input)', default=None)
oparser.add_argument("-m", "--max-occ", help="Maximum number of occurrences of a word in one language to be kept in the index", type=int, dest="maxo", default=-1)

options = oparser.parse_args()

if options.lett != None:
  reader = open(options.lett,"r")
else:
  reader = sys.stdin

docnumber = 0
word_map = {}

punctuation=get_unicode_punct()
#sys.stderr.write(punctuation+"\n")
word_freqs = {}
for line in reader:
  ##################
  #Parsing the text:
  ##################
  fields=line.strip().split("\t")
  lang=fields[0]
  #word_freqs[lang].update(PunktWordTokenizer().tokenize(text.lower()))

  #Decoding base 64:
  text = base64.b64decode(fields[5]).decode("utf-8")
  if lang not in word_freqs:
    word_freqs[lang]=Counter()
  word_freqs[lang].update(word_tokenize(text.lower()))
  #Getting the bag of words in the document
  #word_freqs.update(PunktWordTokenizer().tokenize(text.lower()))
  sorted_uniq_wordlist = set(" ".join(word_tokenize(text)).lower().split())
  #sorted_uniq_wordlist = set(" ".join(PunktWordTokenizer().tokenize(text)).lower().split())
  #Trimming non-aplphanumerics:
  clean_sorted_uniq_wordlist = filter(None, [w.strip(punctuation) for w in sorted_uniq_wordlist])
  sorted_uniq_wordlist=clean_sorted_uniq_wordlist

  for word in sorted_uniq_wordlist:
    if lang in word_map:
      if word in word_map[lang]:
        word_map[lang][word].append(docnumber)
      else:
        word_map[lang][word] = []
        word_map[lang][word].append(docnumber)
    else:
      word_map[lang] = {}
      word_map[lang][word] = []
      word_map[lang][word].append(docnumber)
  docnumber=docnumber+1

word_freqs_norm={}
for lang in word_freqs.keys():
  total=sum(word_freqs[lang].values())
  word_freqs_norm[lang]={}
  for word in word_freqs[lang].keys():
    word_freqs_norm[lang][word]=float(word_freqs[lang][word])/total

for map_lang, map_vocabulary in word_map.items():
  docset = Set([])
  for l in map_vocabulary.values():
    docset.update(l)
  numdocs=len(docset)
  for map_word, map_doc in map_vocabulary.items():
    if options.maxo == -1 or len(word_map[map_lang][map_word])<=options.maxo:
      sorted_docs=sorted(word_map[map_lang][map_word], reverse=True)
      for doc_list_idx in range(0, len(sorted_docs)-1):
        sorted_docs[doc_list_idx]=str(sorted_docs[doc_list_idx]-sorted_docs[doc_list_idx+1])
      sorted_docs[len(sorted_docs)-1]=str(sorted_docs[len(sorted_docs)-1])
      #print map_lang+"\t"+map_word+"\t"+":".join(reversed(sorted_docs))
      if map_word in word_freqs_norm[map_lang]:
        print map_lang+"\t"+map_word+"\t"+":".join(reversed(sorted_docs))+"\t"+str(word_freqs_norm[map_lang][map_word])+"\t"+str(math.log(numdocs/float(len(sorted_docs))))
      else:
        #print map_lang+"\t"+map_word+"\t"+":".join(reversed(sorted_docs))
        sys.stderr.write(map_word+"\n")

